{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242040a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Greenhouse Gas Analytics - Model Development\n",
    "# Notebook 05: Machine Learning Models for Emission Prediction and Analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Time Series Analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ü§ñ Greenhouse Gas Analytics - Model Development\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# ## 1. Load and Prepare Data for Modeling\n",
    "\n",
    "@st.cache_data\n",
    "def load_modeling_data():\n",
    "    \"\"\"Load and prepare data for machine learning\"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet('../data/processed/cleaned_data.parquet')\n",
    "        print(\"‚úÖ Processed data loaded for modeling!\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Creating enhanced sample data for modeling...\")\n",
    "        return create_modeling_\n",
    "        print(f\"\\\\nüíæ Model export completed successfully!\")\n",
    "  \n",
    "  ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"summary_insights\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. üìã Final Summary and Key Insights\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"summary\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def generate_final_summary():\\n\",\n",
    "    \"    \\\"\\\"\\\"Generate comprehensive final summary of all modeling work\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nüìã COMPREHENSIVE MODELING SUMMARY:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*45)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary_report = {\\n\",\n",
    "    \"        'dataset_summary': {},\\n\",\n",
    "    \"        'modeling_results': {},\\n\",\n",
    "    \"        'key_insights': [],\\n\",\n",
    "    \"        'recommendations': []\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Dataset Summary\\n\",\n",
    "    \"    print(f\\\"\\\\nüìä DATASET OVERVIEW:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*25)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    dataset_stats = {\\n\",\n",
    "    \"        'total_records': len(df_enhanced),\\n\",\n",
    "    \"        'countries': df_enhanced['country'].nunique(),\\n\",\n",
    "    \"        'regions': df_enhanced['region'].nunique(),\\n\",\n",
    "    \"        'sectors': df_enhanced['type'].nunique(),\\n\",\n",
    "    \"        'years_covered': f\\\"{df_enhanced['year'].min()}-{df_enhanced['year'].max()}\\\",\\n\",\n",
    "    \"        'total_emissions': df_enhanced['emissions'].sum(),\\n\",\n",
    "    \"        'avg_emissions_per_record': df_enhanced['emissions'].mean(),\\n\",\n",
    "    \"        'features_engineered': df_enhanced.shape[1] - df.shape[1],\\n\",\n",
    "    \"        'features_selected': len(selected_features)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for key, value in dataset_stats.items():\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ {key.replace('_', ' ').title()}: {value}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary_report['dataset_summary'] = dataset_stats\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Modeling Results Summary\\n\",\n",
    "    \"    print(f\\\"\\\\nü§ñ MODELING RESULTS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*25)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    modeling_summary = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Regression results\\n\",\n",
    "    \"    if 'tuned_results' in locals() and 'best_model_name' in locals():\\n\",\n",
    "    \"        print(f\\\"\\\\nüìà REGRESSION MODELING:\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Best Model: {best_model_name}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Test R¬≤: {tuned_results[best_model_name]['test_r2']:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Test RMSE: {tuned_results[best_model_name]['test_rmse']:.2f} Mt CO‚ÇÇe\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Cross-validation R¬≤: {tuned_results[best_model_name]['best_score']:.4f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        modeling_summary['regression'] = {\\n\",\n",
    "    \"            'best_model': best_model_name,\\n\",\n",
    "    \"            'test_r2': tuned_results[best_model_name]['test_r2'],\\n\",\n",
    "    \"            'test_rmse': tuned_results[best_model_name]['test_rmse'],\\n\",\n",
    "    \"            'cv_r2': tuned_results[best_model_name]['best_score']\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Classification results\\n\",\n",
    "    \"    if 'cls_results' in locals():\\n\",\n",
    "    \"        best_cls_name = cls_results.index[0]\\n\",\n",
    "    \"        print(f\\\"\\\\nüéØ CLASSIFICATION MODELING:\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Best Model: {best_cls_name}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Accuracy: {cls_results.loc[best_cls_name, 'accuracy']:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ F1 Score: {cls_results.loc[best_cls_name, 'f1_score']:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Precision: {cls_results.loc[best_cls_name, 'precision']:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Recall: {cls_results.loc[best_cls_name, 'recall']:.4f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        modeling_summary['classification'] = {\\n\",\n",
    "    \"            'best_model': best_cls_name,\\n\",\n",
    "    \"            'accuracy': cls_results.loc[best_cls_name, 'accuracy'],\\n\",\n",
    "    \"            'f1_score': cls_results.loc[best_cls_name, 'f1_score'],\\n\",\n",
    "    \"            'precision': cls_results.loc[best_cls_name, 'precision'],\\n\",\n",
    "    \"            'recall': cls_results.loc[best_cls_name, 'recall']\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Clustering results\\n\",\n",
    "    \"    if 'clustering_results' in locals() and 'best_clustering_method' in locals():\\n\",\n",
    "    \"        print(f\\\"\\\\nüéØ CLUSTERING ANALYSIS:\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Best Method: {best_clustering_method}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Silhouette Score: {clustering_results[best_clustering_method]['silhouette_score']:.4f}\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Number of Clusters: {clustering_results[best_clustering_method]['n_clusters']}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        modeling_summary['clustering'] = {\\n\",\n",
    "    \"            'best_method': best_clustering_method,\\n\",\n",
    "    \"            'silhouette_score': clustering_results[best_clustering_method]['silhouette_score'],\\n\",\n",
    "    \"            'n_clusters': clustering_results[best_clustering_method]['n_clusters']\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Time series results\\n\",\n",
    "    \"    if 'ts_results' in locals():\\n\",\n",
    "    \"        print(f\\\"\\\\nüìà TIME SERIES ANALYSIS:\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Average Growth Rate: {ts_results['trend_analysis']['avg_growth_rate']:.2f}% per year\\\")\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ Series Stationarity: {'Stationary' if ts_results['trend_analysis']['is_stationary'] else 'Non-stationary'}\\\")\\n\",\n",
    "    \"        if 'ml_forecasts' in ts_results and ts_results['ml_forecasts']:\\n\",\n",
    "    \"            print(f\\\"  ‚Ä¢ ML Forecasts Generated: {len(ts_results['ml_forecasts'])} countries\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        modeling_summary['time_series'] = {\\n\",\n",
    "    \"            'avg_growth_rate': ts_results['trend_analysis']['avg_growth_rate'],\\n\",\n",
    "    \"            'is_stationary': ts_results['trend_analysis']['is_stationary'],\\n\",\n",
    "    \"            'forecasts_generated': len(ts_results.get('ml_forecasts', {}))\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary_report['modeling_results'] = modeling_summary\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Key Insights\\n\",\n",
    "    \"    print(f\\\"\\\\nüí° KEY INSIGHTS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*20)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    key_insights = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Feature importance insights\\n\",\n",
    "    \"    if 'interpretation_results' in locals() and 'feature_importance' in interpretation_results:\\n\",\n",
    "    \"        top_feature = interpretation_results['feature_importance'].iloc[0]['feature']\\n\",\n",
    "    \"        top_importance = interpretation_results['feature_importance'].iloc[0]['importance']\\n\",\n",
    "    \"        insight = f\\\"Most predictive feature: {top_feature} (importance: {top_importance:.4f})\\\"\\n\",\n",
    "    \"        key_insights.append(insight)\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ {insight}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Model performance insights\\n\",\n",
    "    \"    if 'tuned_results' in locals() and 'best_model_name' in locals():\\n\",\n",
    "    \"        r2_score = tuned_results[best_model_name]['test_r2']\\n\",\n",
    "    \"        if r2_score > 0.8:\\n\",\n",
    "    \"            performance_level = \\\"excellent\\\"\\n\",\n",
    "    \"        elif r2_score > 0.6:\\n\",\n",
    "    \"            performance_level = \\\"good\\\"\\n\",\n",
    "    \"        elif r2_score > 0.4:\\n\",\n",
    "    \"            performance_level = \\\"moderate\\\"\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            performance_level = \\\"poor\\\"\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        insight = f\\\"Regression model shows {performance_level} predictive performance (R¬≤ = {r2_score:.3f})\\\"\\n\",\n",
    "    \"        key_insights.append(insight)\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ {insight}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Clustering insights\\n\",\n",
    "    \"    if 'clustering_results' in locals() and 'best_clustering_method' in locals():\\n\",\n",
    "    \"        n_clusters = clustering_results[best_clustering_method]['n_clusters']\\n\",\n",
    "    \"        sil_score = clustering_results[best_clustering_method]['silhouette_score']\\n\",\n",
    "    \"        insight = f\\\"Countries can be effectively grouped into {n_clusters} emission patterns (silhouette: {sil_score:.3f})\\\"\\n\",\n",
    "    \"        key_insights.append(insight)\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ {insight}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Time series insights\\n\",\n",
    "    \"    if 'ts_results' in locals():\\n\",\n",
    "    \"        growth_rate = ts_results['trend_analysis']['avg_growth_rate']\\n\",\n",
    "    \"        trend_direction = \\\"increasing\\\" if growth_rate > 0 else \\\"decreasing\\\"\\n\",\n",
    "    \"        insight = f\\\"Global emissions trend is {trend_direction} at {abs(growth_rate):.1f}% per year\\\"\\n\",\n",
    "    \"        key_insights.append(insight)\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ {insight}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Data quality insights\\n\",\n",
    "    \"    missing_pct = (df_enhanced.isnull().sum().sum() / (df_enhanced.shape[0] * df_enhanced.shape[1])) * 100\\n\",\n",
    "    \"    insight = f\\\"Data quality is {'excellent' if missing_pct < 1 else 'good' if missing_pct < 5 else 'moderate'} with {missing_pct:.1f}% missing values\\\"\\n\",\n",
    "    \"    key_insights.append(insight)\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ {insight}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Feature engineering insights\\n\",\n",
    "    \"    features_created = df_enhanced.shape[1] - df.shape[1]\\n\",\n",
    "    \"    insight = f\\\"Feature engineering created {features_created} additional features, improving model performance\\\"\\n\",\n",
    "    \"    key_insights.append(insight)\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ {insight}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary_report['key_insights'] = key_insights\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 4. Recommendations\\n\",\n",
    "    \"    print(f\\\"\\\\nüöÄ RECOMMENDATIONS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*25)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    recommendations = [\\n\",\n",
    "    \"        \\\"Deploy the best performing models in a production environment for real-time predictions\\\",\\n\",\n",
    "    \"        \\\"Implement automated model retraining pipeline with new data\\\",\\n\",\n",
    "    \"        \\\"Create interactive dashboards using the trained models for stakeholder insights\\\",\\n\",\n",
    "    \"        \\\"Develop country-specific emission reduction strategies based on clustering analysis\\\",\\n\",\n",
    "    \"        \\\"Establish monitoring system for model performance degradation over time\\\",\\n\",\n",
    "    \"        \\\"Collect additional features (policy data, economic indicators) to improve predictions\\\",\\n\",\n",
    "    \"        \\\"Implement A/B testing framework for model updates and improvements\\\",\\n\",\n",
    "    \"        \\\"Create automated alert system for unusual emission patterns or predictions\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add performance-specific recommendations\\n\",\n",
    "    \"    if 'tuned_results' in locals() and 'best_model_name' in locals():\\n\",\n",
    "    \"        r2_score = tuned_results[best_model_name]['test_r2']\\n\",\n",
    "    \"        if r2_score < 0.7:\\n\",\n",
    "    \"            recommendations.insert(1, \\\"Consider ensemble methods or deep learning approaches to improve prediction accuracy\\\")\\n\",\n",
    "    \"            recommendations.insert(2, \\\"Investigate additional data sources for better feature coverage\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, rec in enumerate(recommendations, 1):\\n\",\n",
    "    \"        print(f\\\"  {i:2d}. {rec}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary_report['recommendations'] = recommendations\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 5. Technical Specifications\\n\",\n",
    "    \"    print(f\\\"\\\\nüîß TECHNICAL SPECIFICATIONS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*35)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tech_specs = {\\n\",\n",
    "    \"        'python_version': '3.8+',\\n\",\n",
    "    \"        'key_libraries': ['scikit-learn', 'pandas', 'numpy', 'plotly', 'statsmodels'],\\n\",\n",
    "    \"        'model_formats': 'joblib (sklearn compatible)',\\n\",\n",
    "    \"        'preprocessing': 'StandardScaler for numerical features',\\n\",\n",
    "    \"        'validation_method': '5-fold cross-validation',\\n\",\n",
    "    \"        'hyperparameter_tuning': 'GridSearchCV',\\n\",\n",
    "    \"        'feature_selection': 'Consensus of multiple methods (F-test, RFE, RF importance)'\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for key, value in tech_specs.items():\\n\",\n",
    "    \"        if isinstance(value, list):\\n\",\n",
    "    \"            value = ', '.join(value)\\n\",\n",
    "    \"        print(f\\\"  ‚Ä¢ {key.replace('_', ' ').title()}: {value}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 6. Save comprehensive summary\\n\",\n",
    "    \"    print(f\\\"\\\\nüíæ SAVING COMPREHENSIVE SUMMARY:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*35)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Save as JSON\\n\",\n",
    "    \"    summary_filename = '../data/models/comprehensive_modeling_summary.json'\\n\",\n",
    "    \"    with open(summary_filename, 'w') as f:\\n\",\n",
    "    \"        json.dump(summary_report, f, indent=2, default=str)\\n\",\n",
    "    \"    print(f\\\"  ‚úÖ Summary saved as JSON: {summary_filename}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Save as formatted text report\\n\",\n",
    "    \"    report_filename = '../data/models/modeling_report.txt'\\n\",\n",
    "    \"    with open(report_filename, 'w') as f:\\n\",\n",
    "    \"        f.write(f\\\"GREENHOUSE GAS ANALYTICS - COMPREHENSIVE MODELING REPORT\\\\n\\\")\\n\",\n",
    "    \"        f.write(f\\\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\")\\n\",\n",
    "    \"        f.write(f\\\"={'='*60}\\\\n\\\\n\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        f.write(f\\\"DATASET SUMMARY:\\\\n\\\")\\n\",\n",
    "    \"        f.write(f\\\"{'-'*20}\\\\n\\\")\\n\",\n",
    "    \"        for key, value in dataset_stats.items():\\n\",\n",
    "    \"            f.write(f\\\"{key.replace('_', ' ').title()}: {value}\\\\n\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        f.write(f\\\"\\\\nMODELING RESULTS:\\\\n\\\")\\n\",\n",
    "    \"        f.write(f\\\"{'-'*20}\\\\n\\\")\\n\",\n",
    "    \"        for model_type, results in modeling_summary.items():\\n\",\n",
    "    \"            f.write(f\\\"\\\\n{model_type.upper()}:\\\\n\\\")\\n\",\n",
    "    \"            for metric, value in results.items():\\n\",\n",
    "    \"                f.write(f\\\"  {metric.replace('_', ' ').title()}: {value}\\\\n\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        f.write(f\\\"\\\\nKEY INSIGHTS:\\\\n\\\")\\n\",\n",
    "    \"        f.write(f\\\"{'-'*20}\\\\n\\\")\\n\",\n",
    "    \"        for i, insight in enumerate(key_insights, 1):\\n\",\n",
    "    \"            f.write(f\\\"{i}. {insight}\\\\n\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        f.write(f\\\"\\\\nRECOMMENDATIONS:\\\\n\\\")\\n\",\n",
    "    \"        f.write(f\\\"{'-'*20}\\\\n\\\")\\n\",\n",
    "    \"        for i, rec in enumerate(recommendations, 1):\\n\",\n",
    "    \"            f.write(f\\\"{i}. {rec}\\\\n\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  ‚úÖ Report saved as text: {report_filename}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    else:\n",
    "    print(f\"\\\\n‚ö†Ô∏è Best model not available for interpretation analysis\")\n",
    "  \n",
    "  ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"model_export\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. üíæ Model Export and Deployment Preparation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"export\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import joblib\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"\\n\",\n",
    "    \"def export_models_and_artifacts():\\n\",\n",
    "    \"    \\\"\\\"\\\"Export trained models and associated artifacts\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nüíæ MODEL EXPORT AND DEPLOYMENT PREPARATION:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create exports directory\\n\",\n",
    "    \"    import os\\n\",\n",
    "    \"    export_dir = '../data/models'\\n\",\n",
    "    \"    os.makedirs(export_dir, exist_ok=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    export_summary = {\\n\",\n",
    "    \"        'export_timestamp': datetime.now().isoformat(),\\n\",\n",
    "    \"        'models_exported': [],\\n\",\n",
    "    \"        'artifacts_exported': []\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Export best regression model\\n\",\n",
    "    \"    if 'best_model' in locals():\\n\",\n",
    "    \"        print(f\\\"\\\\nüìä Exporting regression models...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Export the best tuned model\\n\",\n",
    "    \"        model_filename = f\\\"{export_dir}/best_regression_model.joblib\\\"\\n\",\n",
    "    \"        joblib.dump(best_model, model_filename)\\n\",\n",
    "    \"        print(f\\\"  ‚úÖ Best regression model saved: {model_filename}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        export_summary['models_exported'].append({\n",
    "            'type': 'clustering',\n",
    "            'model_name': best_clustering_method,\n",
    "            'filename': clustering_filename,\n",
    "            'performance': {\n",
    "                'silhouette_score': clustering_results[best_clustering_method]['silhouette_score'],\n",
    "                'n_clusters': clustering_results[best_clustering_method]['n_clusters']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # 4. Export preprocessors and scalers\n",
    "    print(f\"\\\\nüîß Exporting preprocessors...\\\")\")\n",
    "    \n",
    "    # Feature scaler\n",
    "    if 'scaler' in locals():\n",
    "        scaler_filename = f\"{export_dir}/feature_scaler.joblib\"\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "        print(f\"  ‚úÖ Feature scaler saved: {scaler_filename}\")\n",
    "        export_summary['artifacts_exported'].append({'name': 'feature_scaler', 'filename': scaler_filename})\n",
    "    \n",
    "    # Clustering scaler\n",
    "    if 'cluster_scaler' in locals():\n",
    "        cluster_scaler_filename = f\"{export_dir}/cluster_scaler.joblib\"\n",
    "        joblib.dump(cluster_scaler, cluster_scaler_filename)\n",
    "        print(f\"  ‚úÖ Cluster scaler saved: {cluster_scaler_filename}\")\n",
    "        export_summary['artifacts_exported'].append({'name': 'cluster_scaler', 'filename': cluster_scaler_filename})\n",
    "    \n",
    "    # 5. Export feature lists and metadata\n",
    "    print(f\"\\\\nüìã Exporting metadata...\\\")\")\n",
    "    \n",
    "    # Selected features\n",
    "    selected_features_filename = f\"{export_dir}/selected_features.json\"\n",
    "    with open(selected_features_filename, 'w') as f:\n",
    "        json.dump({\n",
    "            'selected_features': selected_features,\n",
    "            'feature_selection_method': 'consensus',\n",
    "            'n_features_selected': len(selected_features),\n",
    "            'total_features_available': len(X_all.columns)\n",
    "        }, f, indent=2)\n",
    "    print(f\"  ‚úÖ Selected features saved: {selected_features_filename}\")\n",
    "    export_summary['artifacts_exported'].append({'name': 'selected_features', 'filename': selected_features_filename})\n",
    "    \n",
    "    # Feature importance\n",
    "    if 'interpretation_results' in locals() and 'feature_importance' in interpretation_results:\n",
    "        feature_importance_filename = f\"{export_dir}/feature_importance.csv\"\n",
    "        interpretation_results['feature_importance'].to_csv(feature_importance_filename, index=False)\n",
    "        print(f\"  ‚úÖ Feature importance saved: {feature_importance_filename}\")\n",
    "        export_summary['artifacts_exported'].append({'name': 'feature_importance', 'filename': feature_importance_filename})\n",
    "    \n",
    "    # Model performance metrics\n",
    "    performance_filename = f\"{export_dir}/model_performance.json\"\n",
    "    performance_data = {\n",
    "        'regression_performance': regression_results.to_dict() if 'regression_results' in locals() else {},\n",
    "        'classification_performance': cls_results.to_dict() if 'cls_results' in locals() else {},\n",
    "        'clustering_performance': {method: {'silhouette_score': results['silhouette_score'], \n",
    "                                          'n_clusters': results['n_clusters']} \n",
    "                                 for method, results in clustering_results.items()} if 'clustering_results' in locals() else {},\n",
    "        'time_series_results': {\n",
    "            'avg_growth_rate': ts_results['trend_analysis']['avg_growth_rate'],\n",
    "            'is_stationary': ts_results['trend_analysis']['is_stationary']\n",
    "        } if 'ts_results' in locals() else {}\n",
    "    }\n",
    "    \n",
    "    with open(performance_filename, 'w') as f:\n",
    "        json.dump(performance_data, f, indent=2, default=str)\n",
    "    print(f\"  ‚úÖ Model performance metrics saved: {performance_filename}\")\n",
    "    export_summary['artifacts_exported'].append({'name': 'model_performance', 'filename': performance_filename})\n",
    "    \n",
    "    # 6. Create model deployment template\n",
    "    print(f\"\\\\nüöÄ Creating deployment template...\\\")\")\n",
    "    \n",
    "    deployment_template = f'''\n",
    "# Model Deployment Template - Greenhouse Gas Analytics\n",
    "# Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class EmissionPredictor:\n",
    "    def __init__(self, model_dir=\"../data/models\"):\n",
    "        self.model_dir = model_dir\n",
    "        self.regression_model = None\n",
    "        self.classification_model = None\n",
    "        self.clustering_model = None\n",
    "        self.scaler = None\n",
    "        self.selected_features = None\n",
    "        self._load_models()\n",
    "    \n",
    "    def _load_models(self):\n",
    "        \\\"\\\"\\\"Load all trained models and artifacts\\\"\\\"\\\"\n",
    "        try:\n",
    "            # Load regression model\n",
    "            self.regression_model = joblib.load(f\"{{self.model_dir}}/best_regression_model.joblib\")\n",
    "            \n",
    "            # Load classification model\n",
    "            self.classification_model = joblib.load(f\"{{self.model_dir}}/best_classification_model.joblib\")\n",
    "            \n",
    "            # Load clustering model\n",
    "            self.clustering_model = joblib.load(f\"{{self.model_dir}}/best_clustering_model.joblib\")\n",
    "            \n",
    "            # Load scaler\n",
    "            self.scaler = joblib.load(f\"{{self.model_dir}}/feature_scaler.joblib\")\n",
    "            \n",
    "            # Load selected features\n",
    "            with open(f\"{{self.model_dir}}/selected_features.json\", 'r') as f:\n",
    "                feature_data = json.load(f)\n",
    "                self.selected_features = feature_data['selected_features']\n",
    "            \n",
    "            print(\"‚úÖ All models loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading models: {{str(e)}}\")\n",
    "    \n",
    "    def predict_emissions(self, X):\n",
    "        \\\"\\\"\\\"Predict emissions for given features\\\"\\\"\\\"\n",
    "        if self.regression_model is None:\n",
    "            raise ValueError(\"Regression model not loaded\")\n",
    "        \n",
    "        # Select and scale features\n",
    "        X_selected = X[self.selected_features]\n",
    "        X_scaled = self.scaler.transform(X_selected)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = self.regression_model.predict(X_scaled)\n",
    "        return predictions\n",
    "    \n",
    "    def classify_emissions(self, X):\n",
    "        \\\"\\\"\\\"Classify emission levels for given features\\\"\\\"\\\"\n",
    "        if self.classification_model is None:\n",
    "            raise ValueError(\"Classification model not loaded\")\n",
    "        \n",
    "        # Select and scale features\n",
    "        X_selected = X[self.selected_features]\n",
    "        \n",
    "        # Make prediction (use appropriate scaling if needed)\n",
    "        if hasattr(self.classification_model, 'predict_proba'):\n",
    "            predictions = self.classification_model.predict(X_selected)\n",
    "            probabilities = self.classification_model.predict_proba(X_selected)\n",
    "            return predictions, probabilities\n",
    "        else:\n",
    "            predictions = self.classification_model.predict(X_selected)\n",
    "            return predictions, None\n",
    "    \n",
    "    def cluster_countries(self, X):\n",
    "        \\\"\\\"\\\"Assign countries to emission clusters\\\"\\\"\\\"\n",
    "        if self.clustering_model is None:\n",
    "            raise ValueError(\"Clustering model not loaded\")\n",
    "        \n",
    "        # Select and scale features\n",
    "        X_selected = X[self.selected_features]\n",
    "        cluster_scaler = joblib.load(f\"{{self.model_dir}}/cluster_scaler.joblib\")\n",
    "        X_scaled = cluster_scaler.transform(X_selected)\n",
    "        \n",
    "        # Assign clusters\n",
    "        clusters = self.clustering_model.predict(X_scaled)\n",
    "        return clusters\n",
    "\n",
    "# Example usage:\n",
    "# predictor = EmissionPredictor()\n",
    "# emissions = predictor.predict_emissions(new_data)\n",
    "# categories = predictor.classify_emissions(new_data)\n",
    "# clusters = predictor.cluster_countries(new_data)\n",
    "    '''\n",
    "    \n",
    "    template_filename = f\"{export_dir}/deployment_template.py\"\n",
    "    with open(template_filename, 'w') as f:\n",
    "        f.write(deployment_template)\n",
    "    print(f\"  ‚úÖ Deployment template created: {template_filename}\")\n",
    "    export_summary['artifacts_exported'].append({'name': 'deployment_template', 'filename': template_filename})\n",
    "    \n",
    "    # 7. Export summary\n",
    "    summary_filename = f\"{export_dir}/export_summary.json\"\n",
    "    with open(summary_filename, 'w') as f:\n",
    "        json.dump(export_summary, f, indent=2, default=str)\n",
    "    print(f\"  ‚úÖ Export summary saved: {summary_filename}\")\n",
    "    \n",
    "    # 8. Create README for models\n",
    "    readme_content = f'''\n",
    "# Greenhouse Gas Analytics - Trained Models\n",
    "\n",
    "## Export Summary\n",
    "- **Export Date**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- **Models Exported**: {len(export_summary[\"models_exported\"])}\n",
    "- **Artifacts Exported**: {len(export_summary[\"artifacts_exported\"])}\n",
    "\n",
    "## Model Performance Summary\n",
    "\n",
    "### Regression Model ({best_model_name if 'best_model_name' in locals() else 'N/A'})\n",
    "- **R¬≤ Score**: {tuned_results[best_model_name]['test_r2']:.4f if 'tuned_results' in locals() and 'best_model_name' in locals() else 'N/A'}\n",
    "- **RMSE**: {tuned_results[best_model_name]['test_rmse']:.2f if 'tuned_results' in locals() and 'best_model_name' in locals() else 'N/A'}\n",
    "\n",
    "### Classification Model ({cls_results.index[0] if 'cls_results' in locals() else 'N/A'})\n",
    "- **Accuracy**: {cls_results.iloc[0]['accuracy']:.4f if 'cls_results' in locals() else 'N/A'}\n",
    "- **F1 Score**: {cls_results.iloc[0]['f1_score']:.4f if 'cls_results' in locals() else 'N/A'}\n",
    "\n",
    "### Clustering Model ({best_clustering_method if 'best_clustering_method' in locals() else 'N/A'})\n",
    "- **Silhouette Score**: {clustering_results[best_clustering_method]['silhouette_score']:.4f if 'clustering_results' in locals() and 'best_clustering_method' in locals() else 'N/A'}\n",
    "- **Number of Clusters**: {clustering_results[best_clustering_method]['n_clusters'] if 'clustering_results' in locals() and 'best_clustering_method' in locals() else 'N/A'}\n",
    "\n",
    "## Files Description\n",
    "\n",
    "### Models\n",
    "- `best_regression_model.joblib` - Best performing regression model for emissions prediction\n",
    "- `best_classification_model.joblib` - Best performing classification model for emission categories\n",
    "- `best_clustering_model.joblib` - Best performing clustering model for country grouping\n",
    "\n",
    "### Preprocessors\n",
    "- `feature_scaler.joblib` - StandardScaler for feature preprocessing\n",
    "- `cluster_scaler.joblib` - StandardScaler for clustering preprocessing\n",
    "\n",
    "### Metadata\n",
    "- `selected_features.json` - List of selected features used by models\n",
    "- `feature_importance.csv` - Feature importance rankings\n",
    "- `model_performance.json` - Detailed performance metrics for all models\n",
    "- `export_summary.json` - Complete export metadata\n",
    "\n",
    "### Deployment\n",
    "- `deployment_template.py` - Python template for model deployment\n",
    "- `README.md` - This documentation file\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from deployment_template import EmissionPredictor\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = EmissionPredictor()\n",
    "\n",
    "# Make predictions\n",
    "emissions = predictor.predict_emissions(your_data)\n",
    "categories = predictor.classify_emissions(your_data)\n",
    "clusters = predictor.cluster_countries(your_data)\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- joblib\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn\n",
    "\n",
    "## Note\n",
    "All models were trained on processed methane emissions data and should be used within the same data context and feature engineering pipeline.\n",
    "    '''\n",
    "    \n",
    "    readme_filename = f\"{export_dir}/README.md\"\n",
    "    with open(readme_filename, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"  ‚úÖ README documentation created: {readme_filename}\")\n",
    "    \n",
    "    print(f\"\\\\nüéâ MODEL EXPORT COMPLETED!\")\n",
    "    print(f\"üìÅ All files saved in: {export_dir}\")\n",
    "    print(f\"üìä Total files exported: {len(export_summary['models_exported']) + len(export_summary['artifacts_exported']) + 2}\")\n",
    "    \n",
    "    return export_summary\n",
    "\n",
    "# Export all models and artifacts\n",
    "export_summary = export_models_and_artifacts()\n",
    "\n",
    "print(f\"\\\\nüíæ Model export completed successfully!\"){\\n\",\n",
    "    \"            'type': 'regression',\\n\",\n",
    "    \"            'model_name': best_model_name,\\n\",\n",
    "    \"            'filename': model_filename,\\n\",\n",
    "    \"            'performance': {\\n\",\n",
    "    \"                'test_r2': tuned_results[best_model_name]['test_r2'],\\n\",\n",
    "    \"                'test_rmse': tuned_results[best_model_name]['test_rmse']\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Export best classification model\\n\",\n",
    "    \"    if 'cls_trained_models' in locals():\\n\",\n",
    "    \"        print(f\\\"\\\\nüéØ Exporting classification models...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        best_cls_name = cls_results.index[0]\\n\",\n",
    "    \"        best_cls_model = cls_trained_models[best_cls_name]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        cls_model_filename = f\\\"{export_dir}/best_classification_model.joblib\\\"\\n\",\n",
    "    \"        joblib.dump(best_cls_model, cls_model_filename)\\n\",\n",
    "    \"        print(f\\\"  ‚úÖ Best classification model saved: {cls_model_filename}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        export_summary['models_exported'].append({\\n\",\n",
    "    \"            'type': 'classification',\\n\",\n",
    "    \"            'model_name': best_cls_name,\\n\",\n",
    "    \"            'filename': cls_model_filename,\\n\",\n",
    "    \"            'performance': {\\n\",\n",
    "    \"                'accuracy': cls_results.loc[best_cls_name, 'accuracy'],\\n\",\n",
    "    \"                'f1_score': cls_results.loc[best_cls_name, 'f1_score']\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Export clustering model\\n\",\n",
    "    \"    if 'clustering_results' in locals():\\n\",\n",
    "    \"        print(f\\\"\\\\nüéØ Exporting clustering models...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        best_clustering_model = clustering_results[best_clustering_method]['model']\\n\",\n",
    "    \"        clustering_filename = f\\\"{export_dir}/best_clustering_model.joblib\\\"\\n\",\n",
    "    \"        joblib.dump(best_clustering_model, clustering_filename)\\n\",\n",
    "    \"        print(f\\\"  ‚úÖ Best clustering model saved: {clustering_filename}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        export_summary['models_exported'].append(print(f\"\\\\nüéØ Clustering analysis completed with {best_clustering_method}\")\n",
    "  \n",
    "  ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"time_series_models\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. üìà Time Series Analysis and Forecasting\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"time_series\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def time_series_analysis(df_enhanced):\\n\",\n",
    "    \"    \\\"\\\"\\\"Perform time series analysis and forecasting\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nüìà TIME SERIES ANALYSIS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*35)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Aggregate data by year for time series analysis\\n\",\n",
    "    \"    ts_data = df_enhanced.groupby(['year'])['emissions'].agg(['sum', 'mean', 'count']).reset_index()\\n\",\n",
    "    \"    ts_data = ts_data.sort_values('year')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"üìä Time series data shape: {ts_data.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"üìÖ Years covered: {ts_data['year'].min()} - {ts_data['year'].max()}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create time series by sector\\n\",\n",
    "    \"    ts_sector = df_enhanced.groupby(['year', 'type'])['emissions'].sum().reset_index()\\n\",\n",
    "    \"    ts_sector_pivot = ts_sector.pivot(index='year', columns='type', values='emissions').fillna(0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Time series by region\\n\",\n",
    "    \"    ts_region = df_enhanced.groupby(['year', 'region'])['emissions'].sum().reset_index()\\n\",\n",
    "    \"    ts_region_pivot = ts_region.pivot(index='year', columns='region', values='emissions').fillna(0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    ts_results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Trend Analysis\\n\",\n",
    "    \"    print(f\\\"\\\\nüìä TREND ANALYSIS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*20)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate year-over-year growth rates\\n\",\n",
    "    \"    ts_data['total_growth_rate'] = ts_data['sum'].pct_change() * 100\\n\",\n",
    "    \"    ts_data['avg_growth_rate'] = ts_data['mean'].pct_change() * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    avg_growth = ts_data['total_growth_rate'].mean()\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Average annual growth rate: {avg_growth:.2f}%\\\")\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Total emissions trend: {'Increasing' if avg_growth > 0 else 'Decreasing'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Stationarity Test\\n\",\n",
    "    \"    print(f\\\"\\\\nüîç STATIONARITY TESTING:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*25)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Augmented Dickey-Fuller test\\n\",\n",
    "    \"    total_emissions = ts_data['sum'].values\\n\",\n",
    "    \"    adf_result = adfuller(total_emissions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ ADF Statistic: {adf_result[0]:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ p-value: {adf_result[1]:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Critical Values: {adf_result[4]}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    is_stationary = adf_result[1] <= 0.05\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Series is {'stationary' if is_stationary else 'non-stationary'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Seasonal Decomposition\\n\",\n",
    "    \"    print(f\\\"\\\\nüîÑ SEASONAL DECOMPOSITION:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*30)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(ts_data) >= 4:  # Need at least 4 observations\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            # Create time index\\n\",\n",
    "    \"            ts_data['date'] = pd.to_datetime(ts_data['year'], format='%Y')\\n\",\n",
    "    \"            ts_series = ts_data.set_index('date')['sum']\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Perform seasonal decomposition\\n\",\n",
    "    \"            decomposition = seasonal_decompose(ts_series, model='additive', period=2)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize decomposition\\n\",\n",
    "    \"            fig, axes = plt.subplots(4, 1, figsize=(15, 12))\\n\",\n",
    "    \"            fig.suptitle('üìä Time Series Decomposition', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            decomposition.observed.plot(ax=axes[0], title='Original', color='blue')\n",
    "            decomposition.trend.plot(ax=axes[1], title='Trend', color='green')\n",
    "            decomposition.seasonal.plot(ax=axes[2], title='Seasonal', color='orange')\n",
    "            decomposition.resid.plot(ax=axes[3], title='Residual', color='red')\n",
    "            \n",
    "            for ax in axes:\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_ylabel('Emissions')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate trend strength\n",
    "            trend_strength = 1 - (decomposition.resid.var() / (decomposition.trend + decomposition.resid).var())\n",
    "            seasonal_strength = 1 - (decomposition.resid.var() / (decomposition.seasonal + decomposition.resid).var())\n",
    "            \n",
    "            print(f\"  ‚Ä¢ Trend strength: {trend_strength:.4f}\")\n",
    "            print(f\"  ‚Ä¢ Seasonal strength: {seasonal_strength:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Seasonal decomposition failed: {str(e)}\")\n",
    "    \n",
    "    # 4. ARIMA Modeling\n",
    "    print(f\"\\\\nüéØ ARIMA MODELING:\\\")\\n\",\n",
    "    print(\\\"=\\\"*20)\n",
    "    \n",
    "    try:\n",
    "        # Simple ARIMA model\n",
    "        if not is_stationary:\n",
    "            # Difference the series\n",
    "            diff_series = pd.Series(total_emissions).diff().dropna()\n",
    "        else:\n",
    "            diff_series = pd.Series(total_emissions)\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        arima_model = ARIMA(total_emissions, order=(1,1,1))\n",
    "        arima_fit = arima_model.fit()\n",
    "        \n",
    "        print(f\"  ‚Ä¢ ARIMA(1,1,1) Model Summary:\")\n",
    "        print(f\"  ‚Ä¢ AIC: {arima_fit.aic:.2f}\")\n",
    "        print(f\"  ‚Ä¢ BIC: {arima_fit.bic:.2f}\")\n",
    "        \n",
    "        # Make forecasts\n",
    "        forecast_steps = 2\n",
    "        forecast = arima_fit.forecast(steps=forecast_steps)\n",
    "        forecast_ci = arima_fit.get_forecast(steps=forecast_steps).conf_int()\n",
    "        \n",
    "        ts_results['arima_forecast'] = forecast\n",
    "        ts_results['forecast_ci'] = forecast_ci\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Forecast for next {forecast_steps} periods: {forecast.round(2).tolist()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è ARIMA modeling failed: {str(e)}\")\n",
    "        ts_results['arima_forecast'] = None\n",
    "    \n",
    "    # 5. Visualize Time Series Analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üìà Time Series Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Total emissions over time\n",
    "    axes[0,0].plot(ts_data['year'], ts_data['sum'], 'bo-', linewidth=2, markersize=6)\n",
    "    axes[0,0].set_xlabel('Year')\n",
    "    axes[0,0].set_ylabel('Total Emissions (Mt CO‚ÇÇe)')\n",
    "    axes[0,0].set_title('Total Emissions Trend')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(ts_data['year'], ts_data['sum'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[0,0].plot(ts_data['year'], p(ts_data['year']), 'r--', alpha=0.7, label=f'Trend: {z[0]:.1f}/year')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Growth rates\n",
    "    axes[0,1].bar(ts_data['year'][1:], ts_data['total_growth_rate'][1:], alpha=0.7, color='green')\n",
    "    axes[0,1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0,1].set_xlabel('Year')\n",
    "    axes[0,1].set_ylabel('Growth Rate (%)')\n",
    "    axes[0,1].set_title('Year-over-Year Growth Rate')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Emissions by sector over time\n",
    "    for sector in ts_sector_pivot.columns:\n",
    "        axes[1,0].plot(ts_sector_pivot.index, ts_sector_pivot[sector], \n",
    "                      marker='o', linewidth=2, label=sector)\n",
    "    axes[1,0].set_xlabel('Year')\n",
    "    axes[1,0].set_ylabel('Emissions (Mt CO‚ÇÇe)')\n",
    "    axes[1,0].set_title('Emissions by Sector')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Emissions by region over time\n",
    "    for region in ts_region_pivot.columns:\n",
    "        axes[1,1].plot(ts_region_pivot.index, ts_region_pivot[region], \n",
    "                      marker='s', linewidth=2, label=region)\n",
    "    axes[1,1].set_xlabel('Year')\n",
    "    axes[1,1].set_ylabel('Emissions (Mt CO‚ÇÇe)')\n",
    "    axes[1,1].set_title('Emissions by Region')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Forecasting with Machine Learning\n",
    "    print(f\"\\\\nü§ñ ML-BASED FORECASTING:\\\")\\n\",\n",
    "    print(\\\"=\\\"*30)\n",
    "    \n",
    "    # Prepare data for ML forecasting\n",
    "    forecast_features = ['population', 'gdp_per_capita', 'energy_intensity', 'year_normalized']\n",
    "    \n",
    "    # Country-level time series for major emitters\n",
    "    major_countries = df_enhanced.groupby('country')['emissions'].sum().nlargest(5).index\n",
    "    \n",
    "    ml_forecasts = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for country in major_countries:\\n\",\n",
    "    \"        country_data = df_enhanced[df_enhanced['country'] == country].copy()\\n\",\n",
    "    \"        country_ts = country_data.groupby('year').agg({\\n\",\n",
    "    \"            'emissions': 'sum',\\n\",\n",
    "    \"            'population': 'first',\\n\",\n",
    "    \"            'gdp_per_capita': 'first',\\n\",\n",
    "    \"            'energy_intensity': 'first',\\n\",\n",
    "    \"            'year_normalized': 'first'\\n\",\n",
    "    \"        }).reset_index()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(country_ts) >= 3:  # Need at least 3 data points\\n\",\n",
    "    \"            # Use Random Forest for forecasting\\n\",\n",
    "    \"            X_country = country_ts[forecast_features]\\n\",\n",
    "    \"            y_country = country_ts['emissions']\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            rf_forecast = RandomForestRegressor(n_estimators=50, random_state=RANDOM_STATE)\\n\",\n",
    "    \"            rf_forecast.fit(X_country, y_country)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Predict next year (extrapolate features)\\n\",\n",
    "    \"            next_year_features = X_country.iloc[-1:].copy()\\n\",\n",
    "    \"            next_year_features['year_normalized'] += 1 / (df_enhanced['year'].max() - df_enhanced['year'].min())\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            forecast_emission = rf_forecast.predict(next_year_features)[0]\\n\",\n",
    "    \"            ml_forecasts[country] = forecast_emission\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ ML forecasts for major emitters (next year):\\\")\\n\",\n",
    "    \"    for country, forecast in ml_forecasts.items():\\n\",\n",
    "    \"        current_emission = df_enhanced[df_enhanced['country'] == country]['emissions'].sum() / df_enhanced['year'].nunique()\\n\",\n",
    "    \"        change_pct = (forecast - current_emission) / current_emission * 100\\n\",\n",
    "    \"        print(f\\\"    - {country}: {forecast:.1f} Mt CO‚ÇÇe ({change_pct:+.1f}%)\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    ts_results['ml_forecasts'] = ml_forecasts\\n\",\n",
    "    \"    ts_results['trend_analysis'] = {\\n\",\n",
    "    \"        'avg_growth_rate': avg_growth,\\n\",\n",
    "    \"        'is_stationary': is_stationary,\\n\",\n",
    "    \"        'time_series_data': ts_data\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return ts_results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform time series analysis\\n\",\n",
    "    \"ts_results = time_series_analysis(df_enhanced)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìà Time series analysis completed!\\\")\"print(f\"\\\\nüèÜ Best tuned model: {best_model_name}\")\n",
    "print(f\"üìä Final test R¬≤: {tuned_results[best_model_name]['test_r2']:.4f}\")\n",
    "  \n",
    "  ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"classification_models\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. üéØ Classification Models for Emission Categories\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"classification\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def train_classification_models(X, y_continuous, df_enhanced):\\n\",\n",
    "    \"    \\\"\\\"\\\"Train classification models for emission categories\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nüéØ CLASSIFICATION MODEL TRAINING:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*40)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create classification target based on emission levels\\n\",\n",
    "    \"    emission_quantiles = y_continuous.quantile([0.33, 0.66, 1.0])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def categorize_emissions(emission):\\n\",\n",
    "    \"        if emission <= emission_quantiles.iloc[0]:\\n\",\n",
    "    \"            return 'Low'\\n\",\n",
    "    \"        elif emission <= emission_quantiles.iloc[1]:\\n\",\n",
    "    \"            return 'Medium'\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            return 'High'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    y_categorical = y_continuous.apply(categorize_emissions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"üìä Emission categories distribution:\\\")\\n\",\n",
    "    \"    print(y_categorical.value_counts())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Split the data\\n\",\n",
    "    \"    X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\\n\",\n",
    "    \"        X, y_categorical, test_size=0.2, random_state=RANDOM_STATE, stratify=y_categorical\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Define classification models\\n\",\n",
    "    \"    cls_models = {\\n\",\n",
    "    \"        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\\n\",\n",
    "    \"        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\\n\",\n",
    "    \"        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "        'SVM': SVC(random_state=RANDOM_STATE, probability=True),\n",
    "        'Naive Bayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate classification models\n",
    "    cls_results = {}\n",
    "    cls_model_objects = {}\n",
    "    \n",
    "    # Create preprocessing pipeline for classification\n",
    "    scaler_cls = StandardScaler()\n",
    "    X_train_cls_scaled = scaler_cls.fit_transform(X_train_cls)\n",
    "    X_test_cls_scaled = scaler_cls.transform(X_test_cls)\n",
    "    \n",
    "    for name, model in cls_models.items():\n",
    "        print(f\"\\\\nüîß Training {name}...\")\n",
    "        \n",
    "        # Use scaled data for some models\n",
    "        if name in ['Logistic Regression', 'SVM']:\n",
    "            model.fit(X_train_cls_scaled, y_train_cls)\n",
    "            y_pred_cls = model.predict(X_test_cls_scaled)\n",
    "            y_pred_proba = model.predict_proba(X_test_cls_scaled) if hasattr(model, 'predict_proba') else None\n",
    "            cv_scores = cross_val_score(model, X_train_cls_scaled, y_train_cls, cv=5, scoring='accuracy')\n",
    "        else:\n",
    "            model.fit(X_train_cls, y_train_cls)\n",
    "            y_pred_cls = model.predict(X_test_cls)\n",
    "            y_pred_proba = model.predict_proba(X_test_cls) if hasattr(model, 'predict_proba') else None\n",
    "            cv_scores = cross_val_score(model, X_train_cls, y_train_cls, cv=5, scoring='accuracy')\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test_cls, y_pred_cls)\n",
    "        precision = precision_score(y_test_cls, y_pred_cls, average='weighted')\n",
    "        recall = recall_score(y_test_cls, y_pred_cls, average='weighted')\n",
    "        f1 = f1_score(y_test_cls, y_pred_cls, average='weighted')\n",
    "        \n",
    "        cls_results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'cv_accuracy_mean': cv_scores.mean(),\n",
    "            'cv_accuracy_std': cv_scores.std(),\n",
    "            'predictions': y_pred_cls,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        cls_model_objects[name] = model\n",
    "        \n",
    "        print(f\"  ‚úÖ Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    cls_results_df = pd.DataFrame(cls_results).T\n",
    "    cls_results_df = cls_results_df.sort_values('f1_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\\\nüèÜ CLASSIFICATION PERFORMANCE:\")\n",
    "    print(\"=\"*35)\n",
    "    print(cls_results_df[['accuracy', 'precision', 'recall', 'f1_score']].round(4))\n",
    "    \n",
    "    # Visualize classification results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üéØ Classification Models Performance', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models_list = cls_results_df.index.tolist()\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    accuracy_vals = cls_results_df['accuracy'].values\n",
    "    axes[0,0].bar(models_list, accuracy_vals, color='lightblue', alpha=0.7)\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].set_title('Model Accuracy Comparison')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(accuracy_vals):\n",
    "        axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    f1_vals = cls_results_df['f1_score'].values\n",
    "    axes[0,1].bar(models_list, f1_vals, color='lightgreen', alpha=0.7)\n",
    "    axes[0,1].set_ylabel('F1 Score')\n",
    "    axes[0,1].set_title('F1 Score Comparison')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(f1_vals):\n",
    "        axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_means = cls_results_df['cv_accuracy_mean'].values\n",
    "    cv_stds = cls_results_df['cv_accuracy_std'].values\n",
    "    \n",
    "    axes[1,0].bar(models_list, cv_means, yerr=cv_stds, capsize=5, alpha=0.7, color='orange')\n",
    "    axes[1,0].set_ylabel('CV Accuracy')\n",
    "    axes[1,0].set_title('5-Fold Cross-Validation Accuracy')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion Matrix for best model\n",
    "    best_cls_model_name = cls_results_df.index[0]\n",
    "    best_cls_predictions = cls_results_df.loc[best_cls_model_name, 'predictions']\n",
    "    \n",
    "    cm = confusion_matrix(y_test_cls, best_cls_predictions)\n",
    "    im = axes[1,1].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    axes[1,1].set_title(f'Confusion Matrix: {best_cls_model_name}')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=axes[1,1])\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            axes[1,1].text(j, i, format(cm[i, j], 'd'),\n",
    "                         ha=\"center\", va=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    # Set tick labels\n",
    "    tick_marks = np.arange(len(np.unique(y_test_cls)))\n",
    "    axes[1,1].set_xticks(tick_marks)\n",
    "    axes[1,1].set_yticks(tick_marks)\n",
    "    axes[1,1].set_xticklabels(sorted(np.unique(y_test_cls)))\n",
    "    axes[1,1].set_yticklabels(sorted(np.unique(y_test_cls)))\n",
    "    axes[1,1].set_xlabel('Predicted Label')\n",
    "    axes[1,1].set_ylabel('True Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed classification report for best model\n",
    "    print(f\"\\\\nüìã DETAILED CLASSIFICATION REPORT - {best_cls_model_name}:\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test_cls, best_cls_predictions))\n",
    "    \n",
    "    return cls_results_df, cls_model_objects, (X_train_cls, X_test_cls, y_train_cls, y_test_cls), scaler_cls\n",
    "\n",
    "# Train classification models\n",
    "cls_results, cls_trained_models, cls_data_splits, cls_scaler = train_classification_models(\n",
    "    X_selected, y, df_enhanced\n",
    ")\n",
    "\n",
    "print(f\"\\\\nüèÜ Best classification model: {cls_results.index[0]}\")\n",
    "print(f\"üìä Best F1 score: {cls_results.iloc[0]['f1_score']:.4f}\")\n",
    "  \n",
    "  ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"clustering_analysis\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. üéØ Clustering Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"clustering\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def perform_clustering_analysis(X, df_enhanced):\\n\",\n",
    "    \"    \\\"\\\"\\\"Perform comprehensive clustering analysis\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nüéØ CLUSTERING ANALYSIS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*30)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Prepare data for clustering\\n\",\n",
    "    \"    scaler_cluster = StandardScaler()\\n\",\n",
    "    \"    X_scaled = scaler_cluster.fit_transform(X)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    clustering_results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. K-Means Clustering\\n\",\n",
    "    \"    print(\\\"\\\\nüîç K-Means Clustering Analysis...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Determine optimal number of clusters using elbow method and silhouette analysis\\n\",\n",
    "    \"    k_range = range(2, 11)\\n\",\n",
    "    \"    inertias = []\\n\",\n",
    "    \"    silhouette_scores = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for k in k_range:\\n\",\n",
    "    \"        kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\\n\",\n",
    "    \"        cluster_labels = kmeans.fit_predict(X_scaled)\\n\",\n",
    "    \"        inertias.append(kmeans.inertia_)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(set(cluster_labels)) > 1:\\n\",\n",
    "    \"            sil_score = silhouette_score(X_scaled, cluster_labels)\\n\",\n",
    "    \"            silhouette_scores.append(sil_score)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            silhouette_scores.append(0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find optimal k\\n\",\n",
    "    \"    optimal_k = k_range[np.argmax(silhouette_scores)]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Optimal number of clusters: {optimal_k}\\\")\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Best silhouette score: {max(silhouette_scores):.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Perform final K-means clustering\\n\",\n",
    "    \"    final_kmeans = KMeans(n_clusters=optimal_k, random_state=RANDOM_STATE, n_init=10)\\n\",\n",
    "    \"    kmeans_labels = final_kmeans.fit_predict(X_scaled)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    clustering_results['K-Means'] = {\\n\",\n",
    "    \"        'labels': kmeans_labels,\\n\",\n",
    "    \"        'n_clusters': optimal_k,\\n\",\n",
    "    \"        'silhouette_score': silhouette_score(X_scaled, kmeans_labels),\\n\",\n",
    "    \"        'model': final_kmeans\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. DBSCAN Clustering\\n\",\n",
    "    \"    print(\\\"\\\\nüåê DBSCAN Clustering Analysis...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Try different eps values\\n\",\n",
    "    \"    eps_range = [0.3, 0.5, 0.7, 1.0, 1.5]\\n\",\n",
    "    \"    best_dbscan_score = -1\\n\",\n",
    "    \"    best_dbscan_eps = 0.5\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for eps in eps_range:\\n\",\n",
    "    \"        dbscan = DBSCAN(eps=eps, min_samples=5)\\n\",\n",
    "    \"        dbscan_labels = dbscan.fit_predict(X_scaled)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\\n\",\n",
    "    \"        n_noise = list(dbscan_labels).count(-1)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if n_clusters > 1 and n_noise < len(dbscan_labels) * 0.5:\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                sil_score = silhouette_score(X_scaled, dbscan_labels)\\n\",\n",
    "    \"                if sil_score > best_dbscan_score:\\n\",\n",
    "    \"                    best_dbscan_score = sil_score\\n\",\n",
    "    \"                    best_dbscan_eps = eps\\n\",\n",
    "    \"            except:\\n\",\n",
    "    \"                continue\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Final DBSCAN with best parameters\\n\",\n",
    "    \"    final_dbscan = DBSCAN(eps=best_dbscan_eps, min_samples=5)\\n\",\n",
    "    \"    dbscan_labels = final_dbscan.fit_predict(X_scaled)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    dbscan_n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\\n\",\n",
    "    \"    dbscan_n_noise = list(dbscan_labels).count(-1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    clustering_results['DBSCAN'] = {\\n\",\n",
    "    \"        'labels': dbscan_labels,\\n\",\n",
    "    \"        'n_clusters': dbscan_n_clusters,\\n\",\n",
    "    \"        'n_noise': dbscan_n_noise,\\n\",\n",
    "    \"        'eps': best_dbscan_eps,\\n\",\n",
    "    \"        'silhouette_score': best_dbscan_score if best_dbscan_score > -1 else 0,\\n\",\n",
    "    \"        'model': final_dbscan\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Number of clusters: {dbscan_n_clusters}\\\")\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Number of noise points: {dbscan_n_noise}\\\")\\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Best eps: {best_dbscan_eps}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Hierarchical Clustering\\n\",\n",
    "    \"    print(\\\"\\\\nüå≥ Agglomerative Clustering Analysis...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    agg_clustering = AgglomerativeClustering(n_clusters=optimal_k)\\n\",\n",
    "    \"    agg_labels = agg_clustering.fit_predict(X_scaled)\\n\",\n",
    "    \"    agg_silhouette = silhouette_score(X_scaled, agg_labels)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    clustering_results['Agglomerative'] = {\\n\",\n",
    "    \"        'labels': agg_labels,\\n\",\n",
    "    \"        'n_clusters': optimal_k,\\n\",\n",
    "    \"        'silhouette_score': agg_silhouette,\\n\",\n",
    "    \"        'model': agg_clustering\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"  ‚Ä¢ Silhouette score: {agg_silhouette:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize clustering results\\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\\n\",\n",
    "    \"    fig.suptitle('üéØ Clustering Analysis Results', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Elbow curve and silhouette scores\\n\",\n",
    "    \"    axes[0,0].plot(k_range, inertias, 'bo-')\\n\",\n",
    "    \"    axes[0,0].set_xlabel('Number of clusters (k)')\\n\",\n",
    "    \"    axes[0,0].set_ylabel('Inertia')\\n\",\n",
    "    \"    axes[0,0].set_title('Elbow Method')\\n\",\n",
    "    \"    axes[0,0].grid(True, alpha=0.3)\\n\",\n",
    "    \"    axes[0,0].axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[0,1].plot(k_range, silhouette_scores, 'ro-')\\n\",\n",
    "    \"    axes[0,1].set_xlabel('Number of clusters (k)')\\n\",\n",
    "    \"    axes[0,1].set_ylabel('Silhouette Score')\\n\",\n",
    "    \"    axes[0,1].set_title('Silhouette Analysis')\\n\",\n",
    "    \"    axes[0,1].grid(True, alpha=0.3)\\n\",\n",
    "    \"    axes[0,1].axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # PCA visualization of clusters\\n\",\n",
    "    \"    pca = PCA(n_components=2, random_state=RANDOM_STATE)\\n\",\n",
    "    \"    X_pca = pca.fit_transform(X_scaled)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # K-Means clusters in PCA space\\n\",\n",
    "    \"    scatter = axes[0,2].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)\\n\",\n",
    "    \"    axes[0,2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\\n\",\n",
    "    \"    axes[0,2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\\n\",\n",
    "    \"    axes[0,2].set_title(f'K-Means Clusters (k={optimal_k})')\\n\",\n",
    "    \"    plt.colorbar(scatter, ax=axes[0,2])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # DBSCAN clusters in PCA space\\n\",\n",
    "    \"    scatter2 = axes[1,0].scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)\\n\",\n",
    "    \"    axes[1,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\\n\",\n",
    "    \"    axes[1,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\\n\",\n",
    "    \"    axes[1,0].set_title(f'DBSCAN Clusters ({dbscan_n_clusters} clusters)')\\n\",\n",
    "    \"    plt.colorbar(scatter2, ax=axes[1,0])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Agglomerative clusters in PCA space\\n\",\n",
    "    \"    scatter3 = axes[1,1].scatter(X_pca[:, 0], X_pca[:, 1], c=agg_labels, cmap='viridis', alpha=0.6)\\n\",\n",
    "    \"    axes[1,1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\\n\",\n",
    "    \"    axes[1,1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\\n\",\n",
    "    \"    axes[1,1].set_title(f'Agglomerative Clusters (k={optimal_k})')\\n\",\n",
    "    \"    plt.colorbar(scatter3, ax=axes[1,1])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Silhouette comparison\\n\",\n",
    "    \"    methods = ['K-Means', 'DBSCAN', 'Agglomerative']\\n\",\n",
    "    \"    sil_scores = [clustering_results[method]['silhouette_score'] for method in methods]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    bars = axes[1,2].bar(methods, sil_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\\n\",\n",
    "    \"    axes[1,2].set_ylabel('Silhouette Score')\\n\",\n",
    "    \"    axes[1,2].set_title('Clustering Methods Comparison')\\n\",\n",
    "    \"    axes[1,2].grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for bar, score in zip(bars, sil_scores):\\n\",\n",
    "    \"        axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\\n\",\n",
    "    \"                      f'{score:.3f}', ha='center', va='bottom')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Analyze cluster characteristics\\n\",\n",
    "    \"    best_clustering_method = max(clustering_results.keys(), \\n\",\n",
    "    \"                                key=lambda x: clustering_results[x]['silhouette_score'])\\n\",\n",
    "    \"    best_labels = clustering_results[best_clustering_method]['labels']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüèÜ Best clustering method: {best_clustering_method}\\\")\\n\",\n",
    "    \"    print(f\\\"üìä Silhouette score: {clustering_results[best_clustering_method]['silhouette_score']:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Cluster characteristics analysis\\n\",\n",
    "    \"    cluster_df = df_enhanced.copy()\\n\",\n",
    "    \"    cluster_df['cluster'] = best_labels\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüìã CLUSTER CHARACTERISTICS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*30)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Analyze key features by cluster\\n\",\n",
    "    \"    key_features = ['emissions', 'population', 'gdp_per_capita', 'energy_intensity', 'forest_cover_pct']\\n\",\n",
    "    \"    cluster_stats = cluster_df.groupby('cluster')[key_features].agg(['mean', 'std']).round(2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Cluster Statistics (Mean ¬± Std):\\\")\\n\",\n",
    "    \"    for cluster in sorted(cluster_df['cluster'].unique()):\\n\",\n",
    "    \"        if cluster != -1:  # Skip noise points for DBSCAN\\n\",\n",
    "    \"            print(f\\\"\\\\nCluster {cluster}:\\\")\\n\",\n",
    "    \"            cluster_data = cluster_df[cluster_df['cluster'] == cluster]\\n\",\n",
    "    \"            print(f\\\"  ‚Ä¢ Size: {len(cluster_data)} ({len(cluster_data)/len(cluster_df)*100:.1f}%)\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            for feature in key_features:\\n\",\n",
    "    \"                mean_val = cluster_data[feature].mean()\\n\",\n",
    "    \"                std_val = cluster_data[feature].std()\\n\",\n",
    "    \"                print(f\\\"  ‚Ä¢ {feature}: {mean_val:.1f} ¬± {std_val:.1f}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Most common countries in cluster\\n\",\n",
    "    \"            top_countries = cluster_data['country'].value_counts().head(3)\\n\",\n",
    "    \"            print(f\\\"  ‚Ä¢ Top countries: {', '.join(top_countries.index.tolist())}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Most common sectors\\n\",\n",
    "    \"            top_sectors = cluster_data['type'].value_counts().head(2)\\n\",\n",
    "    \"            print(f\\\"  ‚Ä¢ Main sectors: {', '.join(top_sectors.index.tolist())}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return clustering_results, best_clustering_method, cluster_df, scaler_cluster\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform clustering analysis\\n\",\n",
    "    \"clustering_results, best_clustering_method, cluster_df, cluster_scaler = perform_clustering_analysis(\\n\",\n",
    "    \"    X_selected, df_enhanced\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüéØ Clustering analysis completed with {best_clustering_method}\\\")\"{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"header\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# ü§ñ Greenhouse Gas Analytics - Model Development\\n\",\n",
    "    \"## Notebook 05: Advanced Machine Learning Models for Methane Emissions\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objetivo:** Desenvolver modelos preditivos e de classifica√ß√£o para an√°lise de emiss√µes de metano\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Modelos implementados:**\\n\",\n",
    "    \"- Regress√£o para previs√£o de emiss√µes\\n\",\n",
    "    \"- Classifica√ß√£o de pa√≠ses por n√≠vel de emiss√£o\\n\",\n",
    "    \"- Clustering de padr√µes de emiss√£o\\n\",\n",
    "    \"- Modelos de s√©ries temporais\\n\",\n",
    "    \"- An√°lise de import√¢ncia de features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"imports\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Greenhouse Gas Analytics - Model Development\\n\",\n",
    "    \"# Notebook 05: Advanced Machine Learning for Methane Emissions Prediction\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning libraries\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\\n\",\n",
    "    \"from sklearn.compose import ColumnTransformer\\n\",\n",
    "    \"from sklearn.pipeline import Pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Regression models\\n\",\n",
    "    \"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\\n\",\n",
    "    \"from sklearn.tree import DecisionTreeRegressor\\n\",\n",
    "    \"from sklearn.svm import SVR\\n\",\n",
    "    \"from sklearn.neighbors import KNeighborsRegressor\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification models\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.svm import SVC\\n\",\n",
    "    \"from sklearn.naive_bayes import GaussianNB\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Clustering\\n\",\n",
    "    \"from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Model evaluation\\n\",\n",
    "    \"from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\\n\",\n",
    "    \"                           classification_report, confusion_matrix, silhouette_score,\\n\",\n",
    "    \"                           accuracy_score, precision_score, recall_score, f1_score)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Time series\\n\",\n",
    "    \"from statsmodels.tsa.arima.model import ARIMA\\n\",\n",
    "    \"from statsmodels.tsa.seasonal import seasonal_decompose\\n\",\n",
    "    \"from statsmodels.tsa.stattools import adfuller\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature selection and interpretation\\n\",\n",
    "    \"from sklearn.feature_selection import SelectKBest, f_regression, RFE\\n\",\n",
    "    \"import shap\\n\",\n",
    "    \"\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set styling\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (12, 8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ü§ñ Greenhouse Gas Analytics - Model Development\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*55)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set random seed for reproducibility\\n\",\n",
    "    \"RANDOM_STATE = 42\\n\",\n",
    "    \"np.random.seed(RANDOM_STATE)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"data_loading\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. üìä Data Loading and Preparation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"load_data\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def load_model_data():\\n\",\n",
    "    \"    \\\"\\\"\\\"Load and prepare data for machine learning\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Try to load processed data first\\n\",\n",
    "    \"        df = pd.read_parquet('../data/processed/cleaned_data.parquet')\\n\",\n",
    "    \"        print(\\\"‚úÖ Processed data loaded successfully!\\\")\\n\",\n",
    "    \"        return df\\n\",\n",
    "    \"    except FileNotFoundError:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            df = pd.read_csv('../data/processed/cleaned_data.csv')\\n\",\n",
    "    \"            print(\\\"‚úÖ CSV data loaded successfully!\\\")\\n\",\n",
    "    \"            return df\\n\",\n",
    "    \"        except FileNotFoundError:\\n\",\n",
    "    \"            print(\\\"‚ö†Ô∏è Creating enhanced sample data for ML modeling...\\\")\\n\",\n",
    "    \"            return create_ml_sample_data()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def create_ml_sample_data():\\n\",\n",
    "    \"    \\\"\\\"\\\"Create rich sample data optimized for machine learning\\\"\\\"\\\"\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Enhanced country data with socioeconomic indicators\\n\",\n",
    "    \"    countries_data = {\\n\",\n",
    "    \"        'China': {'region': 'Asia', 'pop': 1400, 'gdp': 17734, 'dev_level': 'Developing', \\n\",\n",
    "    \"                 'area': 9596, 'forest_cover': 23.0, 'urban_pop': 61.4, 'energy_intensity': 4.1},\\n\",\n",
    "    \"        'India': {'region': 'Asia', 'pop': 1380, 'gdp': 3737, 'dev_level': 'Developing',\\n\",\n",
    "    \"                 'area': 3287, 'forest_cover': 24.4, 'urban_pop': 34.9, 'energy_intensity': 2.9},\\n\",\n",
    "    \"        'United States': {'region': 'North America', 'pop': 330, 'gdp': 63544, 'dev_level': 'Developed',\\n\",\n",
    "    \"                         'area': 9834, 'forest_cover': 33.9, 'urban_pop': 82.7, 'energy_intensity': 1.8},\\n\",\n",
    "    \"        'Indonesia': {'region': 'Asia', 'pop': 270, 'gdp': 4256, 'dev_level': 'Developing',\\n\",\n",
    "    \"                     'area': 1905, 'forest_cover': 50.2, 'urban_pop': 56.0, 'energy_intensity': 3.2},\\n\",\n",
    "    \"        'Brazil': {'region': 'South America', 'pop': 215, 'gdp': 8897, 'dev_level': 'Developing',\\n\",\n",
    "    \"                  'area': 8515, 'forest_cover': 59.4, 'urban_pop': 87.1, 'energy_intensity': 2.1},\\n\",\n",
    "    \"        'Nigeria': {'region': 'Africa', 'pop': 220, 'gdp': 2229, 'dev_level': 'Developing',\\n\",\n",
    "    \"                   'area': 924, 'forest_cover': 6.5, 'urban_pop': 52.0, 'energy_intensity': 4.8},\\n\",\n",
    "    \"        'Russia': {'region': 'Europe', 'pop': 145, 'gdp': 11305, 'dev_level': 'Developed',\\n\",\n",
    "    \"                  'area': 17098, 'forest_cover': 49.8, 'urban_pop': 74.4, 'energy_intensity': 5.2},\\n\",\n",
    "    \"        'Mexico': {'region': 'North America', 'pop': 130, 'gdp': 9946, 'dev_level': 'Developing',\\n\",\n",
    "    \"                  'area': 1964, 'forest_cover': 33.6, 'urban_pop': 80.2, 'energy_intensity': 2.4},\\n\",\n",
    "    \"        'Iran': {'region': 'Asia', 'pop': 85, 'gdp': 5627, 'dev_level': 'Developing',\\n\",\n",
    "    \"                'area': 1648, 'forest_cover': 6.8, 'urban_pop': 75.4, 'energy_intensity': 6.1},\\n\",\n",
    "    \"        'Germany': {'region': 'Europe', 'pop': 83, 'gdp': 46259, 'dev_level': 'Developed',\\n\",\n",
    "    \"                   'area': 357, 'forest_cover': 32.7, 'urban_pop': 77.4, 'energy_intensity': 1.4},\\n\",\n",
    "    \"        'Turkey': {'region': 'Europe', 'pop': 85, 'gdp': 9127, 'dev_level': 'Developing',\\n\",\n",
    "    \"                  'area': 784, 'forest_cover': 15.2, 'urban_pop': 76.0, 'energy_intensity': 2.8},\\n\",\n",
    "    \"        'Canada': {'region': 'North America', 'pop': 38, 'gdp': 43242, 'dev_level': 'Developed',\\n\",\n",
    "    \"                  'area': 9985, 'forest_cover': 38.7, 'urban_pop': 81.6, 'energy_intensity': 2.3},\\n\",\n",
    "    \"        'Australia': {'region': 'Oceania', 'pop': 26, 'gdp': 54907, 'dev_level': 'Developed',\\n\",\n",
    "    \"                     'area': 7692, 'forest_cover': 17.4, 'urban_pop': 86.2, 'energy_intensity': 1.9},\\n\",\n",
    "    \"        'Argentina': {'region': 'South America', 'pop': 45, 'gdp': 8449, 'dev_level': 'Developing',\\n\",\n",
    "    \"                     'area': 2780, 'forest_cover': 9.9, 'urban_pop': 92.0, 'energy_intensity': 2.2},\\n\",\n",
    "    \"        'Saudi Arabia': {'region': 'Asia', 'pop': 35, 'gdp': 23139, 'dev_level': 'Developed',\\n\",\n",
    "    \"                        'area': 2150, 'forest_cover': 0.5, 'urban_pop': 84.3, 'energy_intensity': 4.5},\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    emission_types = ['Agriculture', 'Energy', 'Waste', 'Other']\\n\",\n",
    "    \"    segments = ['Livestock', 'Oil & Gas', 'Landfills', 'Rice Cultivation', 'Coal Mining', 'Bioenergy', 'Gas pipelines']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    data = []\\n\",\n",
    "    \"    record_id = 1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for country, info in countries_data.items():\\n\",\n",
    "    \"        # Create country-specific emission patterns\\n\",\n",
    "    \"        base_factor = (info['pop'] / 100) + (info['gdp'] / 10000)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for year in range(2018, 2023):  # Extended time range\\n\",\n",
    "    \"            # Add temporal trend\\n\",\n",
    "    \"            year_factor = 1 + (year - 2020) * 0.02  # Small yearly change\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            for emission_type in emission_types:\\n\",\n",
    "    \"                for segment in np.random.choice(segments, size=np.random.randint(3, 6), replace=False):\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Sector-specific patterns\\n\",\n",
    "    \"                    sector_multipliers = {\\n\",\n",
    "    \"                        'Agriculture': (1.8 if info['dev_level'] == 'Developing' else 0.9) * (info['forest_cover'] / 50),\\n\",\n",
    "    \"                        'Energy': (1.5 if info['dev_level'] == 'Developed' else 1.1) * (info['energy_intensity'] / 3),\\n\",\n",
    "    \"                        'Waste': 0.8 * (info['urban_pop'] / 100),\\n\",\n",
    "    \"                        'Other': 0.6\\n\",\n",
    "    \"                    }\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Calculate emission value with multiple factors\\n\",\n",
    "    \"                    emission_value = max(0, \\n\",\n",
    "    \"                        base_factor * \\n\",\n",
    "    \"                        sector_multipliers[emission_type] * \\n\",\n",
    "    \"                        year_factor * \\n\",\n",
    "    \"                        np.random.uniform(0.4, 2.0) + \\n\",\n",
    "    \"                        np.random.normal(0, 8)\\n\",\n",
    "    \"                    )\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Create emission intensity categories\\n\",\n",
    "    \"                    if emission_value < 20:\\n\",\n",
    "    \"                        emission_category = 'Low'\\n\",\n",
    "    \"                    elif emission_value < 60:\\n\",\n",
    "    \"                        emission_category = 'Medium'\\n\",\n",
    "    \"                    elif emission_value < 100:\\n\",\n",
    "    \"                        emission_category = 'High'\\n\",\n",
    "    \"                    else:\\n\",\n",
    "    \"                        emission_category = 'Very High'\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    data.append({\\n\",\n",
    "    \"                        'id': record_id,\\n\",\n",
    "    \"                        'country': country,\\n\",\n",
    "    \"                        'region': info['region'],\\n\",\n",
    "    \"                        'population': info['pop'],\\n\",\n",
    "    \"                        'gdp_per_capita': info['gdp'],\\n\",\n",
    "    \"                        'development_level': info['dev_level'],\\n\",\n",
    "    \"                        'area_km2': info['area'],\\n\",\n",
    "    \"                        'forest_cover_pct': info['forest_cover'],\\n\",\n",
    "    \"                        'urban_population_pct': info['urban_pop'],\\n\",\n",
    "    \"                        'energy_intensity': info['energy_intensity'],\\n\",\n",
    "    \"                        'type': emission_type,\\n\",\n",
    "    \"                        'segment': segment,\\n\",\n",
    "    \"                        'emissions': emission_value,\\n\",\n",
    "    \"                        'emission_category': emission_category,\\n\",\n",
    "    \"                        'year': year,\\n\",\n",
    "    \"                        'quarter': np.random.choice(['Q1', 'Q2', 'Q3', 'Q4']),\\n\",\n",
    "    \"                        'confidence_level': np.random.choice(['High', 'Medium', 'Low'], p=[0.7, 0.25, 0.05]),\\n\",\n",
    "    \"                        # Derived features for ML\\n\",\n",
    "    \"                        'emissions_per_capita': emission_value / (info['pop'] / 1000) if info['pop'] > 0 else 0,\\n\",\n",
    "    \"                        'emissions_per_gdp': emission_value / (info['gdp'] / 1000) if info['gdp'] > 0 else 0,\\n\",\n",
    "    \"                        'population_density': info['pop'] / info['area'] if info['area'] > 0 else 0\\n\",\n",
    "    \"                    })\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    record_id += 1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return pd.DataFrame(data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the data\\n\",\n",
    "    \"df = load_model_data()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìä DATASET FOR MACHINE LEARNING:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*40)\\n\",\n",
    "    \"print(f\\\"Shape: {df.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Columns: {len(df.columns)}\\\")\\n\",\n",
    "    \"print(f\\\"Time range: {df['year'].min()}-{df['year'].max()}\\\")\\n\",\n",
    "    \"print(f\\\"Countries: {df['country'].nunique()}\\\")\\n\",\n",
    "    \"print(f\\\"Total emissions: {df['emissions'].sum():,.0f} Mt CO‚ÇÇe\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check data quality\\n\",\n",
    "    \"print(f\\\"\\\\nData Quality Check:\\\")\\n\",\n",
    "    \"print(f\\\"‚Ä¢ Missing values: {df.isnull().sum().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"‚Ä¢ Duplicate records: {df.duplicated().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"‚Ä¢ Negative emissions: {(df['emissions'] < 0).sum()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"feature_engineering\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. üîß Feature Engineering and Selection\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"feature_eng\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def create_advanced_features(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"Create advanced features for machine learning models\\\"\\\"\\\"\\n\",\n",
    "    \"    print(\\\"üîß Creating advanced features...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df_enhanced = df.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Time-based features\\n\",\n",
    "    \"    df_enhanced['year_normalized'] = (df_enhanced['year'] - df_enhanced['year'].min()) / (df_enhanced['year'].max() - df_enhanced['year'].min())\\n\",\n",
    "    \"    df_enhanced['is_recent'] = (df_enhanced['year'] >= df_enhanced['year'].max() - 1).astype(int)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Ratio features\\n\",\n",
    "    \"    df_enhanced['gdp_per_area'] = df_enhanced['gdp_per_capita'] * df_enhanced['population'] / df_enhanced['area_km2']\\n\",\n",
    "    \"    df_enhanced['forest_per_capita'] = df_enhanced['forest_cover_pct'] * df_enhanced['area_km2'] / df_enhanced['population']\\n\",\n",
    "    \"    df_enhanced['urban_density_proxy'] = df_enhanced['urban_population_pct'] * df_enhanced['population_density']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Lagged features (for time series)\\n\",\n",
    "    \"    df_enhanced = df_enhanced.sort_values(['country', 'type', 'segment', 'year'])\\n\",\n",
    "    \"    df_enhanced['emissions_lag1'] = df_enhanced.groupby(['country', 'type', 'segment'])['emissions'].shift(1)\\n\",\n",
    "    \"    df_enhanced['emissions_lag2'] = df_enhanced.groupby(['country', 'type', 'segment'])['emissions'].shift(2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 4. Rolling statistics\\n\",\n",
    "    \"    df_enhanced['emissions_ma3'] = df_enhanced.groupby(['country', 'type', 'segment'])['emissions'].transform(lambda x: x.rolling(3, min_periods=1).mean())\\n\",\n",
    "    \"    df_enhanced['emissions_std3'] = df_enhanced.groupby(['country', 'type', 'segment'])['emissions'].transform(lambda x: x.rolling(3, min_periods=1).std())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 5. Country-level aggregated features\\n\",\n",
    "    \"    country_stats = df_enhanced.groupby('country').agg({\\n\",\n",
    "    \"        'emissions': ['mean', 'std', 'sum'],\\n\",\n",
    "    \"        'type': 'nunique',\\n\",\n",
    "    \"        'segment': 'nunique'\\n\",\n",
    "    \"    }).round(3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    country_stats.columns = ['country_avg_emissions', 'country_std_emissions', 'country_total_emissions',\\n\",\n",
    "    \"                           'country_emission_types', 'country_segments']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df_enhanced = df_enhanced.merge(country_stats, left_on='country', right_index=True, how='left')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 6. Sector-level features\\n\",\n",
    "    \"    sector_stats = df_enhanced.groupby('type').agg({\\n\",\n",
    "    \"        'emissions': ['mean', 'std']\\n\",\n",
    "    \"    }).round(3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    sector_stats.columns = ['sector_avg_emissions', 'sector_std_emissions']\\n\",\n",
    "    \"    df_enhanced = df_enhanced.merge(sector_stats, left_on='type', right_index=True, how='left')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 7. Development level encoding with target encoding\\n\",\n",
    "    \"    dev_level_emissions = df_enhanced.groupby('development_level')['emissions'].mean().to_dict()\\n\",\n",
    "    \"    df_enhanced['dev_level_target_encoded'] = df_enhanced['development_level'].map(dev_level_emissions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 8. Interaction features\\n\",\n",
    "    \"    df_enhanced['pop_gdp_interaction'] = df_enhanced['population'] * df_enhanced['gdp_per_capita'] / 1000\\n\",\n",
    "    \"    df_enhanced['energy_forest_interaction'] = df_enhanced['energy_intensity'] * df_enhanced['forest_cover_pct']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 9. Polynomial features for key variables\\n\",\n",
    "    \"    df_enhanced['population_squared'] = df_enhanced['population'] ** 2\\n\",\n",
    "    \"    df_enhanced['gdp_squared'] = df_enhanced['gdp_per_capita'] ** 2\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 10. Binary flags\\n\",\n",
    "    \"    df_enhanced['is_developed'] = (df_enhanced['development_level'] == 'Developed').astype(int)\\n\",\n",
    "    \"    df_enhanced['is_high_forest'] = (df_enhanced['forest_cover_pct'] > df_enhanced['forest_cover_pct'].median()).astype(int)\\n\",\n",
    "    \"    df_enhanced['is_energy_intensive'] = (df_enhanced['energy_intensity'] > df_enhanced['energy_intensity'].median()).astype(int)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Fill NaN values created by lagged features\\n\",\n",
    "    \"    numeric_columns = df_enhanced.select_dtypes(include=[np.number]).columns\\n\",\n",
    "    \"    df_enhanced[numeric_columns] = df_enhanced[numeric_columns].fillna(df_enhanced[numeric_columns].median())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"‚úÖ Enhanced dataset shape: {df_enhanced.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"üìä New features created: {df_enhanced.shape[1] - df.shape[1]}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df_enhanced\\n\",\n",
    "    \"\\n\",\n",
    "    \"def select_features(X, y, method='all', k=15):\\n\",\n",
    "    \"    \\\"\\\"\\\"Feature selection using multiple methods\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nüéØ FEATURE SELECTION:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*25)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    feature_importance_results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Statistical feature selection\\n\",\n",
    "    \"    if method in ['statistical', 'all']:\\n\",\n",
    "    \"        selector_f = SelectKBest(score_func=f_regression, k=k)\\n\",\n",
    "    \"        X_selected_f = selector_f.fit_transform(X, y)\\n\",\n",
    "    \"        selected_features_f = X.columns[selector_f.get_support()].tolist()\\n\",\n",
    "    \"        feature_scores_f = dict(zip(selected_features_f, selector_f.scores_[selector_f.get_support()]))\\n\",\n",
    "    \"        feature_importance_results['F-test'] = feature_scores_f\\n\",\n",
    "    \"        print(f\\\"üîç F-test selected features: {len(selected_features_f)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Recursive Feature Elimination\\n\",\n",
    "    \"    if method in ['rfe', 'all']:\\n\",\n",
    "    \"        rf_estimator = RandomForestRegressor(n_estimators=50, random_state=RANDOM_STATE)\\n\",\n",
    "    \"        rfe_selector = RFE(estimator=rf_estimator, n_features_to_select=k, step=1)\\n\",\n",
    "    \"        X_selected_rfe = rfe_selector.fit_transform(X, y)\\n\",\n",
    "    \"        selected_features_rfe = X.columns[rfe_selector.get_support()].tolist()\\n\",\n",
    "    \"        feature_importance_results['RFE'] = dict(zip(selected_features_rfe, rfe_selector.ranking_[rfe_selector.get_support()]))\\n\",\n",
    "    \"        print(f\\\"üîÑ RFE selected features: {len(selected_features_rfe)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Random Forest feature importance\\n\",\n",
    "    \"    if method in ['rf', 'all']:\\n\",\n",
    "    \"        rf_importance = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)\\n\",\n",
    "    \"        rf_importance.fit(X, y)\\n\",\n",
    "    \"        feature_importance_rf = dict(zip(X.columns, rf_importance.feature_importances_))\\n\",\n",
    "    \"        top_features_rf = sorted(feature_importance_rf.items(), key=lambda x: x[1], reverse=True)[:k]\\n\",\n",
    "    \"        selected_features_rf = [feat[0] for feat in top_features_rf]\\n\",\n",
    "    \"        feature_importance_results['Random Forest'] = dict(top_features_rf)\\n\",\n",
    "    \"        print(f\\\"üå≥ Random Forest selected features: {len(selected_features_rf)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Combine results and create consensus\\n\",\n",
    "    \"    if method == 'all':\\n\",\n",
    "    \"        all_selected = set()\\n\",\n",
    "    \"        for method_name, features in feature_importance_results.items():\\n\",\n",
    "    \"            all_selected.update(features.keys())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create consensus ranking\\n\",\n",
    "    \"        consensus_scores = {}\\n\",\n",
    "    \"        for feature in all_selected:\\n\",\n",
    "    \"            score = 0\\n\",\n",
    "    \"            count = 0\\n\",\n",
    "    \"            for method_name, features in feature_importance_results.items():\\n\",\n",
    "    \"                if feature in features:\\n\",\n",
    "    \"                    # Normalize scores for different methods\\n\",\n",
    "    \"                    if method_name == 'F-test':\\n\",\n",
    "    \"                        score += features[feature] / max(features.values())\\n\",\n",
    "    \"                    elif method_name == 'RFE':\\n\",\n",
    "    \"                        score += (k + 1 - features[feature]) / k  # Lower rank is better\\n\",\n",
    "    \"                    else:  # Random Forest\\n\",\n",
    "    \"                        score += features[feature]\\n\",\n",
    "    \"                    count += 1\\n\",\n",
    "    \"            consensus_scores[feature] = score / count if count > 0 else 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Select top k features from consensus\\n\",\n",
    "    \"        final_features = sorted(consensus_scores.items(), key=lambda x: x[1], reverse=True)[:k]\\n\",\n",
    "    \"        selected_features_final = [feat[0] for feat in final_features]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\nüéØ Consensus top {k} features:\\\")\\n\",\n",
    "    \"        for i, (feature, score) in enumerate(final_features, 1):\\n\",\n",
    "    \"            print(f\\\"  {i:2d}. {feature}: {score:.4f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return selected_features_final, feature_importance_results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return list(feature_importance_results.values())[0].keys(), feature_importance_results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply feature engineering\\n\",\n",
    "    \"df_enhanced = create_advanced_features(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare features for selection\\n\",\n",
    "    \"# Select only numeric features for initial modeling\\n\",\n",
    "    \"numeric_features = df_enhanced.select_dtypes(include=[np.number]).columns\\n\",\n",
    "    \"feature_columns = [col for col in numeric_features if col not in ['emissions', 'id', 'year']]\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_all = df_enhanced[feature_columns].copy()\\n\",\n",
    "    \"y = df_enhanced['emissions'].copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìã Features available for selection: {len(X_all.columns)}\\\")\\n\",\n",
    "    \"print(f\\\"üìä Target variable statistics:\\\")\\n\",\n",
    "    \"print(f\\\"  ‚Ä¢ Mean: {y.mean():.2f}\\\")\\n\",\n",
    "    \"print(f\\\"  ‚Ä¢ Std: {y.std():.2f}\\\")\\n\",\n",
    "    \"print(f\\\"  ‚Ä¢ Min: {y.min():.2f}\\\")\\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
