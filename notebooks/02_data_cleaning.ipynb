{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae8351",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Greenhouse Gas Analytics - Data Cleaning and Preprocessing\n",
    "# Notebook 02: Comprehensive Data Cleaning Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üßπ Greenhouse Gas Analytics - Data Cleaning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ## 1. Load Raw Data\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"Load raw data with error handling\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('../data/raw/Methane_final.csv')\n",
    "        print(f\"‚úÖ Raw data loaded successfully!\")\n",
    "        print(f\"üìä Original shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Raw data file not found. Creating sample data...\")\n",
    "        return create_sample_data()\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create comprehensive sample dataset for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Realistic country and region mapping\n",
    "    countries_regions = {\n",
    "        'China': 'Asia', 'India': 'Asia', 'United States': 'North America', \n",
    "        'Indonesia': 'Asia', 'Brazil': 'South America', 'Nigeria': 'Africa',\n",
    "        'Russia': 'Europe', 'Mexico': 'North America', 'Iran': 'Asia',\n",
    "        'Germany': 'Europe', 'Turkey': 'Europe', 'Canada': 'North America',\n",
    "        'Australia': 'Oceania', 'Argentina': 'South America', 'Algeria': 'Africa',\n",
    "        'Kazakhstan': 'Asia', 'Uzbekistan': 'Asia', 'Thailand': 'Asia',\n",
    "        'Malaysia': 'Asia', 'Venezuela': 'South America', 'Saudi Arabia': 'Asia',\n",
    "        'Pakistan': 'Asia', 'Egypt': 'Africa', 'Ukraine': 'Europe',\n",
    "        'Bangladesh': 'Asia', 'Vietnam': 'Asia', 'Philippines': 'Asia',\n",
    "        'Myanmar': 'Asia', 'Poland': 'Europe', 'South Africa': 'Africa'\n",
    "    }\n",
    "    \n",
    "    # Emission types and realistic segments\n",
    "    emission_data = {\n",
    "        'Agriculture': ['Livestock', 'Rice Cultivation', 'Agricultural Waste', 'Enteric Fermentation'],\n",
    "        'Energy': ['Oil & Gas', 'Coal Mining', 'Gas pipelines', 'Onshore oil', 'Bioenergy'],\n",
    "        'Waste': ['Landfills', 'Wastewater Treatment', 'Solid Waste', 'Composting'],\n",
    "        'Other': ['Industrial Processes', 'Transportation', 'Fugitive Emissions', 'Total']\n",
    "    }\n",
    "    \n",
    "    base_years = ['2019-2021', '2020-2021', '2022', '2021', '2019', '2020']\n",
    "    \n",
    "    data = []\n",
    "    for country, region in countries_regions.items():\n",
    "        # Generate multiple records per country\n",
    "        num_records = np.random.randint(15, 30)\n",
    "        \n",
    "        for _ in range(num_records):\n",
    "            # Select emission type and appropriate segment\n",
    "            emission_type = np.random.choice(list(emission_data.keys()))\n",
    "            segment = np.random.choice(emission_data[emission_type])\n",
    "            \n",
    "            # Generate realistic emissions based on country and type\n",
    "            base_emission = np.random.exponential(30) + np.random.normal(25, 15)\n",
    "            \n",
    "            # Add some data quality issues intentionally\n",
    "            if np.random.random() < 0.02:  # 2% missing emissions\n",
    "                emission_value = np.nan\n",
    "            elif np.random.random() < 0.01:  # 1% negative values (errors)\n",
    "                emission_value = -np.random.uniform(0, 10)\n",
    "            else:\n",
    "                emission_value = max(0, base_emission)\n",
    "            \n",
    "            # Add some inconsistencies in naming\n",
    "            if np.random.random() < 0.05:  # 5% inconsistent country names\n",
    "                country_name = country.upper() if np.random.random() < 0.5 else country.lower()\n",
    "            else:\n",
    "                country_name = country\n",
    "            \n",
    "            data.append({\n",
    "                'region': region,\n",
    "                'country': country_name,\n",
    "                'emissions': emission_value,\n",
    "                'type': emission_type,\n",
    "                'segment': segment,\n",
    "                'reason': 'All',\n",
    "                'baseYear': np.random.choice(base_years)\n",
    "            })\n",
    "    \n",
    "    # Add some completely null rows (1%)\n",
    "    null_rows = int(len(data) * 0.01)\n",
    "    for _ in range(null_rows):\n",
    "        data.append({col: np.nan for col in ['region', 'country', 'emissions', 'type', 'segment', 'reason', 'baseYear']})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load the data\n",
    "df_raw = load_raw_data()\n",
    "\n",
    "print(f\"\\nüìã RAW DATA OVERVIEW:\")\n",
    "print(\"=\"*25)\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# ## 2. Initial Data Quality Assessment\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    print(f\"\\nüîç DATA QUALITY ASSESSMENT:\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    quality_report = {}\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    \n",
    "    print(f\"üìä MISSING VALUES:\")\n",
    "    for col in df.columns:\n",
    "        if missing_data[col] > 0:\n",
    "            print(f\"  ‚Ä¢ {col}: {missing_data[col]} ({missing_percent[col]:.1f}%)\")\n",
    "    \n",
    "    quality_report['missing_values'] = missing_data.to_dict()\n",
    "    \n",
    "    # Duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ DUPLICATES: {duplicates} rows\")\n",
    "    quality_report['duplicates'] = duplicates\n",
    "    \n",
    "    # Data type issues\n",
    "    print(f\"\\nüìù DATA TYPES:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  ‚Ä¢ {col}: {df[col].dtype}\")\n",
    "    \n",
    "    # Specific validations for emissions data\n",
    "    if 'emissions' in df.columns:\n",
    "        df['emissions_numeric'] = pd.to_numeric(df['emissions'], errors='coerce')\n",
    "        \n",
    "        negative_emissions = (df['emissions_numeric'] < 0).sum()\n",
    "        zero_emissions = (df['emissions_numeric'] == 0).sum()\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  EMISSIONS VALIDATIONS:\")\n",
    "        print(f\"  ‚Ä¢ Negative emissions: {negative_emissions}\")\n",
    "        print(f\"  ‚Ä¢ Zero emissions: {zero_emissions}\")\n",
    "        \n",
    "        quality_report['negative_emissions'] = negative_emissions\n",
    "        quality_report['zero_emissions'] = zero_emissions\n",
    "    \n",
    "    # Categorical data inconsistencies\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    print(f\"\\nüè∑Ô∏è  CATEGORICAL DATA:\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            unique_vals = df[col].nunique()\n",
    "            sample_vals = df[col].dropna().unique()[:5]\n",
    "            print(f\"  ‚Ä¢ {col}: {unique_vals} unique values\")\n",
    "            print(f\"    Sample: {list(sample_vals)}\")\n",
    "            \n",
    "            # Check for case inconsistencies\n",
    "            if df[col].dtype == 'object':\n",
    "                original_count = df[col].nunique()\n",
    "                standardized_count = df[col].str.strip().str.title().nunique()\n",
    "                if original_count != standardized_count:\n",
    "                    print(f\"    ‚ö†Ô∏è  Potential case inconsistencies detected\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "quality_report = assess_data_quality(df_raw)\n",
    "\n",
    "# ## 3. Data Cleaning Pipeline\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"Comprehensive data cleaning pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.cleaning_log = []\n",
    "        \n",
    "    def log_step(self, step, details):\n",
    "        \"\"\"Log cleaning steps\"\"\"\n",
    "        self.cleaning_log.append({\n",
    "            'step': step,\n",
    "            'details': details,\n",
    "            'shape_after': self.df.shape,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        print(f\"‚úÖ {step}: {details}\")\n",
    "    \n",
    "    def clean_column_names(self):\n",
    "        \"\"\"Standardize column names\"\"\"\n",
    "        original_cols = list(self.df.columns)\n",
    "        \n",
    "        # Strip whitespace and standardize\n",
    "        self.df.columns = self.df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "        \n",
    "        # Create mapping for display purposes\n",
    "        col_mapping = dict(zip(original_cols, self.df.columns))\n",
    "        \n",
    "        self.log_step(\"Column Names\", f\"Standardized {len(original_cols)} column names\")\n",
    "        return col_mapping\n",
    "    \n",
    "    def handle_missing_values(self, strategy='smart'):\n",
    "        \"\"\"Handle missing values with smart strategies\"\"\"\n",
    "        initial_missing = self.df.isnull().sum().sum()\n",
    "        \n",
    "        if strategy == 'smart':\n",
    "            # Drop rows where all values are missing\n",
    "            self.df = self.df.dropna(how='all')\n",
    "            \n",
    "            # Handle emissions column specifically\n",
    "            if 'emissions' in self.df.columns:\n",
    "                # Convert to numeric first\n",
    "                self.df['emissions'] = pd.to_numeric(self.df['emissions'], errors='coerce')\n",
    "                \n",
    "                # For emissions, we can't meaningfully impute, so drop missing values\n",
    "                emissions_missing_before = self.df['emissions'].isnull().sum()\n",
    "                self.df = self.df.dropna(subset=['emissions'])\n",
    "                emissions_dropped = emissions_missing_before - self.df['emissions'].isnull().sum()\n",
    "                \n",
    "                if emissions_dropped > 0:\n",
    "                    print(f\"  ‚Ä¢ Dropped {emissions_dropped} rows with missing emissions\")\n",
    "            \n",
    "            # Handle categorical missing values\n",
    "            categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols:\n",
    "                missing_count = self.df[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    # Fill with 'Unknown' for categorical variables\n",
    "                    self.df[col] = self.df[col].fillna('Unknown')\n",
    "                    print(f\"  ‚Ä¢ Filled {missing_count} missing values in {col} with 'Unknown'\")\n",
    "        \n",
    "        final_missing = self.df.isnull().sum().sum()\n",
    "        self.log_step(\"Missing Values\", f\"Reduced from {initial_missing} to {final_missing} missing values\")\n",
    "    \n",
    "    def standardize_categorical_values(self):\n",
    "        \"\"\"Standardize categorical values\"\"\"\n",
    "        changes_made = 0\n",
    "        \n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in self.df.columns:\n",
    "                original_unique = self.df[col].nunique()\n",
    "                \n",
    "                # Strip whitespace and standardize case\n",
    "                self.df[col] = self.df[col].astype(str).str.strip()\n",
    "                \n",
    "                # Specific standardizations\n",
    "                if col == 'country':\n",
    "                    # Standardize country names\n",
    "                    country_mapping = {\n",
    "                        'united states': 'United States',\n",
    "                        'usa': 'United States',\n",
    "                        'us': 'United States',\n",
    "                        'uk': 'United Kingdom',\n",
    "                        'uae': 'United Arab Emirates'\n",
    "                    }\n",
    "                    \n",
    "                    for old_name, new_name in country_mapping.items():\n",
    "                        mask = self.df[col].str.lower() == old_name.lower()\n",
    "                        if mask.any():\n",
    "                            self.df.loc[mask, col] = new_name\n",
    "                            changes_made += mask.sum()\n",
    "                \n",
    "                # Apply title case for most categorical columns\n",
    "                if col not in ['baseyear', 'reason']:\n",
    "                    self.df[col] = self.df[col].str.title()\n",
    "                \n",
    "                new_unique = self.df[col].nunique()\n",
    "                if original_unique != new_unique:\n",
    "                    print(f\"  ‚Ä¢ {col}: {original_unique} ‚Üí {new_unique} unique values\")\n",
    "                    changes_made += (original_unique - new_unique)\n",
    "        \n",
    "        self.log_step(\"Categorical Standardization\", f\"Made {changes_made} standardization changes\")\n",
    "    \n",
    "    def handle_outliers(self, method='iqr', factor=1.5):\n",
    "        \"\"\"Handle outliers in emissions data\"\"\"\n",
    "        if 'emissions' not in self.df.columns:\n",
    "            return\n",
    "        \n",
    "        initial_count = len(self.df)\n",
    "        \n",
    "        if method == 'iqr':\n",
    "            Q1 = self.df['emissions'].quantile(0.25)\n",
    "            Q3 = self.df['emissions'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - factor * IQR\n",
    "            upper_bound = Q3 + factor * IQR\n",
    "            \n",
    "            # Identify outliers\n",
    "            outliers_mask = (self.df['emissions'] < lower_bound) | (self.df['emissions'] > upper_bound)\n",
    "            outliers_count = outliers_mask.sum()\n",
    "            \n",
    "            # Instead of removing, let's cap the values\n",
    "            self.df.loc[self.df['emissions'] < lower_bound, 'emissions'] = lower_bound\n",
    "            self.df.loc[self.df['emissions'] > upper_bound, 'emissions'] = upper_bound\n",
    "            \n",
    "            self.log_step(\"Outlier Handling\", f\"Capped {outliers_count} outliers using IQR method\")\n",
    "        \n",
    "        # Handle negative emissions (clear data errors)\n",
    "        negative_count = (self.df['emissions'] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            self.df = self.df[self.df['emissions'] >= 0]\n",
    "            self.log_step(\"Negative Emissions\", f\"Removed {negative_count} rows with negative emissions\")\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Remove duplicate rows\"\"\"\n",
    "        initial_count = len(self.df)\n",
    "        \n",
    "        # Remove exact duplicates\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        \n",
    "        duplicates_removed = initial_count - len(self.df)\n",
    "        \n",
    "        if duplicates_removed > 0:\n",
    "            self.log_step(\"Duplicate Removal\", f\"Removed {duplicates_removed} duplicate rows\")\n",
    "        else:\n",
    "            self.log_step(\"Duplicate Check\", \"No duplicates found\")\n",
    "    \n",
    "    def create_derived_features(self):\n",
    "        \"\"\"Create useful derived features\"\"\"\n",
    "        features_created = 0\n",
    "        \n",
    "        # Extract year from baseYear\n",
    "        if 'baseyear' in self.df.columns:\n",
    "            # Handle different baseYear formats\n",
    "            self.df['year'] = self.df['baseyear'].str.extract('(\\d{4})', expand=False)\n",
    "            self.df['year'] = pd.to_numeric(self.df['year'], errors='coerce')\n",
    "            \n",
    "            # For ranges like \"2019-2021\", take the end year\n",
    "            range_mask = self.df['baseyear'].str.contains('-', na=False)\n",
    "            if range_mask.any():\n",
    "                range_years = self.df.loc[range_mask, 'baseyear'].str.extract('(\\d{4})-(\\d{4})')\n",
    "                self.df.loc[range_mask, 'year'] = pd.to_numeric(range_years[1])  # Take end year\n",
    "            \n",
    "            features_created += 1\n",
    "            print(f\"  ‚Ä¢ Created 'year' from 'baseyear'\")\n",
    "        \n",
    "        # Create emissions categories\n",
    "        if 'emissions' in self.df.columns:\n",
    "            # Categorize emissions into low, medium, high\n",
    "            emissions_quantiles = self.df['emissions'].quantile([0.33, 0.67])\n",
    "            \n",
    "            def categorize_emissions(value):\n",
    "                if pd.isna(value):\n",
    "                    return 'Unknown'\n",
    "                elif value <= emissions_quantiles.iloc[0]:\n",
    "                    return 'Low'\n",
    "                elif value <= emissions_quantiles.iloc[1]:\n",
    "                    return 'Medium'\n",
    "                else:\n",
    "                    return 'High'\n",
    "            \n",
    "            self.df['emissions_category'] = self.df['emissions'].apply(categorize_emissions)\n",
    "            features_created += 1\n",
    "            print(f\"  ‚Ä¢ Created 'emissions_category'\")\n",
    "        \n",
    "        # Create regional groupings\n",
    "        if 'region' in self.df.columns:\n",
    "            # Group regions for analysis\n",
    "            region_groups = {\n",
    "                'Asia': 'Asia-Pacific',\n",
    "                'Oceania': 'Asia-Pacific',\n",
    "                'Europe': 'Europe',\n",
    "                'North America': 'Americas',\n",
    "                'South America': 'Americas',\n",
    "                'Africa': 'Africa'\n",
    "            }\n",
    "            \n",
    "            self.df['region_group'] = self.df['region'].map(region_groups).fillna('Other')\n",
    "            features_created += 1\n",
    "            print(f\"  ‚Ä¢ Created 'region_group'\")\n",
    "        \n",
    "        self.log_step(\"Feature Engineering\", f\"Created {features_created} derived features\")\n",
    "    \n",
    "    def validate_cleaned_data(self):\n",
    "        \"\"\"Validate the cleaned dataset\"\"\"\n",
    "        print(f\"\\n‚úÖ DATA VALIDATION:\")\n",
    "        print(\"=\"*20)\n",
    "        \n",
    "        validation_results = {}\n",
    "        \n",
    "        # Check data completeness\n",
    "        completeness = (1 - self.df.isnull().sum() / len(self.df)) * 100\n",
    "        print(f\"Data Completeness by Column:\")\n",
    "        for col in self.df.columns:\n",
    "            print(f\"  ‚Ä¢ {col}: {completeness[col]:.1f}%\")\n",
    "        \n",
    "        validation_results['completeness'] = completeness.to_dict()\n",
    "        \n",
    "        # Check emissions data validity\n",
    "        if 'emissions' in self.df.columns:\n",
    "            emissions_stats = {\n",
    "                'count': self.df['emissions'].count(),\n",
    "                'mean': self.df['emissions'].mean(),\n",
    "                'std': self.df['emissions'].std(),\n",
    "                'min': self.df['emissions'].min(),\n",
    "                'max': self.df['emissions'].max(),\n",
    "                'negative_count': (self.df['emissions'] < 0).sum(),\n",
    "                'zero_count': (self.df['emissions'] == 0).sum()\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nEmissions Data Validation:\")\n",
    "            print(f\"  ‚Ä¢ Valid records: {emissions_stats['count']}\")\n",
    "            print(f\"  ‚Ä¢ Range: {emissions_stats['min']:.2f} - {emissions_stats['max']:.2f}\")\n",
    "            print(f\"  ‚Ä¢ Mean: {emissions_stats['mean']:.2f}\")\n",
    "            print(f\"  ‚Ä¢ Negative values: {emissions_stats['negative_count']}\")\n",
    "            print(f\"  ‚Ä¢ Zero values: {emissions_stats['zero_count']}\")\n",
    "            \n",
    "            validation_results['emissions_stats'] = emissions_stats\n",
    "        \n",
    "        # Check categorical data consistency\n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        print(f\"\\nCategorical Data Summary:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = self.df[col].nunique()\n",
    "            print(f\"  ‚Ä¢ {col}: {unique_count} unique values\")\n",
    "            \n",
    "            # Show top categories\n",
    "            top_categories = self.df[col].value_counts().head(3)\n",
    "            for category, count in top_categories.items():\n",
    "                pct = (count / len(self.df)) * 100\n",
    "                print(f\"    - {category}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Overall data quality score\n",
    "        avg_completeness = completeness.mean()\n",
    "        has_negatives = validation_results.get('emissions_stats', {}).get('negative_count', 0) > 0\n",
    "        \n",
    "        if avg_completeness >= 95 and not has_negatives:\n",
    "            quality_score = \"Excellent\"\n",
    "        elif avg_completeness >= 90 and not has_negatives:\n",
    "            quality_score = \"Good\"\n",
    "        elif avg_completeness >= 80:\n",
    "            quality_score = \"Fair\"\n",
    "        else:\n",
    "            quality_score = \"Poor\"\n",
    "        \n",
    "        print(f\"\\nüèÜ Overall Data Quality: {quality_score}\")\n",
    "        print(f\"üìä Average Completeness: {avg_completeness:.1f}%\")\n",
    "        \n",
    "        validation_results['quality_score'] = quality_score\n",
    "        validation_results['avg_completeness'] = avg_completeness\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def get_cleaning_summary(self):\n",
    "        \"\"\"Get summary of cleaning operations\"\"\"\n",
    "        return {\n",
    "            'steps_performed': len(self.cleaning_log),\n",
    "            'final_shape': self.df.shape,\n",
    "            'cleaning_log': self.cleaning_log\n",
    "        }\n",
    "\n",
    "# ## 4. Execute Data Cleaning Pipeline\n",
    "\n",
    "print(f\"\\nüßπ EXECUTING CLEANING PIPELINE:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = DataCleaner(df_raw)\n",
    "\n",
    "# Execute cleaning steps\n",
    "print(f\"\\n1Ô∏è‚É£ Cleaning column names...\")\n",
    "col_mapping = cleaner.clean_column_names()\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ Handling missing values...\")\n",
    "cleaner.handle_missing_values()\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ Standardizing categorical values...\")\n",
    "cleaner.standardize_categorical_values()\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£ Handling outliers...\")\n",
    "cleaner.handle_outliers()\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£ Removing duplicates...\")\n",
    "cleaner.remove_duplicates()\n",
    "\n",
    "print(f\"\\n6Ô∏è‚É£ Creating derived features...\")\n",
    "cleaner.create_derived_features()\n",
    "\n",
    "print(f\"\\n7Ô∏è‚É£ Validating cleaned data...\")\n",
    "validation_results = cleaner.validate_cleaned_data()\n",
    "\n",
    "# Get the cleaned dataframe\n",
    "df_clean = cleaner.df.copy()\n",
    "\n",
    "# ## 5. Before/After Comparison\n",
    "\n",
    "def create_comparison_plots(df_before, df_after):\n",
    "    \"\"\"Create before/after comparison visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Data Cleaning: Before vs After Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Data completeness comparison\n",
    "    before_completeness = (1 - df_before.isnull().sum() / len(df_before)) * 100\n",
    "    after_completeness = (1 - df_after.isnull().sum() / len(df_after)) * 100\n",
    "    \n",
    "    x_pos = np.arange(len(before_completeness))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,0].bar(x_pos - width/2, before_completeness.values, width, \n",
    "                  label='Before', alpha=0.8, color='red')\n",
    "    axes[0,0].bar(x_pos + width/2, after_completeness.values, width, \n",
    "                  label='After', alpha=0.8, color='green')\n",
    "    axes[0,0].set_title('Data Completeness by Column')\n",
    "    axes[0,0].set_ylabel('Completeness %')\n",
    "    axes[0,0].set_xticks(x_pos)\n",
    "    axes[0,0].set_xticklabels(before_completeness.index, rotation=45)\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Dataset size comparison\n",
    "    sizes = ['Before Cleaning', 'After Cleaning']\n",
    "    counts = [len(df_before), len(df_after)]\n",
    "    colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    axes[0,1].bar(sizes, counts, color=colors, alpha=0.8)\n",
    "    axes[0,1].set_title('Dataset Size Comparison')\n",
    "    axes[0,1].set_ylabel('Number of Records')\n",
    "    for i, v in enumerate(counts):\n",
    "        axes[0,1].text(i, v + max(counts)*0.01, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Emissions distribution comparison (if available)\n",
    "    if 'emissions' in df_before.columns and 'emissions' in df_after.columns:\n",
    "        # Convert to numeric for comparison\n",
    "        emissions_before = pd.to_numeric(df_before['emissions'], errors='coerce').dropna()\n",
    "        emissions_after = pd.to_numeric(df_after['emissions'], errors='coerce').dropna()\n",
    "        \n",
    "        axes[0,2].hist(emissions_before, bins=30, alpha=0.7, label='Before', color='red', density=True)\n",
    "        axes[0,2].hist(emissions_after, bins=30, alpha=0.7, label='After', color='green', density=True)\n",
    "        axes[0,2].set_title('Emissions Distribution')\n",
    "        axes[0,2].set_xlabel('Emissions (Mt CO‚ÇÇe)')\n",
    "        axes[0,2].set_ylabel('Density')\n",
    "        axes[0,2].legend()\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Missing values heatmap - Before\n",
    "    missing_before = df_before.isnull()\n",
    "    if missing_before.any().any():\n",
    "        sns.heatmap(missing_before.transpose(), cbar=True, cmap='Reds', \n",
    "                   ax=axes[1,0], xticklabels=False)\n",
    "        axes[1,0].set_title('Missing Values Pattern - Before')\n",
    "    else:\n",
    "        axes[1,0].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', \n",
    "                       transform=axes[1,0].transAxes, fontsize=12)\n",
    "        axes[1,0].set_title('Missing Values Pattern - Before')\n",
    "    \n",
    "    # 5. Missing values heatmap - After\n",
    "    missing_after = df_after.isnull()\n",
    "    if missing_after.any().any():\n",
    "        sns.heatmap(missing_after.transpose(), cbar=True, cmap='Greens', \n",
    "                   ax=axes[1,1], xticklabels=False)\n",
    "        axes[1,1].set_title('Missing Values Pattern - After')\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', \n",
    "                       transform=axes[1,1].transAxes, fontsize=12)\n",
    "        axes[1,1].set_title('Missing Values Pattern - After')\n",
    "    \n",
    "    # 6. Data quality metrics\n",
    "    metrics = ['Records', 'Completeness %', 'Unique Countries', 'Unique Regions']\n",
    "    before_metrics = [\n",
    "        len(df_before),\n",
    "        before_completeness.mean(),\n",
    "        df_before['country'].nunique() if 'country' in df_before.columns else 0,\n",
    "        df_before['region'].nunique() if 'region' in df_before.columns else 0\n",
    "    ]\n",
    "    after_metrics = [\n",
    "        len(df_after),\n",
    "        after_completeness.mean(),\n",
    "        df_after['country'].nunique() if 'country' in df_after.columns else 0,\n",
    "        df_after['region'].nunique() if 'region' in df_after.columns else 0\n",
    "    ]\n",
    "    \n",
    "    x_pos = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Normalize metrics for comparison (except for the first one)\n",
    "    normalized_before = [before_metrics[0]/1000] + [m for m in before_metrics[1:]]\n",
    "    normalized_after = [after_metrics[0]/1000] + [m for m in after_metrics[1:]]\n",
    "    \n",
    "    axes[1,2].bar(x_pos - width/2, normalized_before, width, \n",
    "                  label='Before', alpha=0.8, color='red')\n",
    "    axes[1,2].bar(x_pos + width/2, normalized_after, width, \n",
    "                  label='After', alpha=0.8, color='green')\n",
    "    axes[1,2].set_title('Data Quality Metrics')\n",
    "    axes[1,2].set_xticks(x_pos)\n",
    "    axes[1,2].set_xticklabels(['Records\\n(x1000)'] + metrics[1:], rotation=0)\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nüìä CREATING COMPARISON VISUALIZATIONS...\")\n",
    "create_comparison_plots(df_raw, df_clean)\n",
    "\n",
    "# ## 6. Data Quality Report\n",
    "\n",
    "def generate_quality_report(df_before, df_after, cleaning_log):\n",
    "    \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'cleaning_timestamp': datetime.now(),\n",
    "        'original_data': {\n",
    "            'shape': df_before.shape,\n",
    "            'missing_values': df_before.isnull().sum().to_dict(),\n",
    "            'duplicates': df_before.duplicated().sum(),\n",
    "            'data_types': df_before.dtypes.to_dict()\n",
    "        },\n",
    "        'cleaned_data': {\n",
    "            'shape': df_after.shape,\n",
    "            'missing_values': df_after.isnull().sum().to_dict(),\n",
    "            'duplicates': df_after.duplicated().sum(),\n",
    "            'data_types': df_after.dtypes.to_dict()\n",
    "        },\n",
    "        'cleaning_operations': cleaning_log,\n",
    "        'improvements': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate improvements\n",
    "    original_completeness = (1 - df_before.isnull().sum().sum() / (df_before.shape[0] * df_before.shape[1])) * 100\n",
    "    cleaned_completeness = (1 - df_after.isnull().sum().sum() / (df_after.shape[0] * df_after.shape[1])) * 100\n",
    "    \n",
    "    report['improvements'] = {\n",
    "        'completeness_improvement': cleaned_completeness - original_completeness,\n",
    "        'records_change': df_after.shape[0] - df_before.shape[0],\n",
    "        'duplicates_removed': df_before.duplicated().sum() - df_after.duplicated().sum()\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate quality report\n",
    "quality_report = generate_quality_report(df_raw, df_clean, cleaner.cleaning_log)\n",
    "\n",
    "print(f\"\\nüìã FINAL DATA QUALITY REPORT:\")\n",
    "print(\"=\"*35)\n",
    "print(f\"üïê Cleaning completed at: {quality_report['cleaning_timestamp']}\")\n",
    "print(f\"\\nüìä Data Changes:\")\n",
    "print(f\"  ‚Ä¢ Original shape: {quality_report['original_data']['shape']}\")\n",
    "print(f\"  ‚Ä¢ Final shape: {quality_report['cleaned_data']['shape']}\")\n",
    "print(f\"  ‚Ä¢ Records change: {quality_report['improvements']['records_change']:+d}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Quality Improvements:\")\n",
    "print(f\"  ‚Ä¢ Completeness: {quality_report['improvements']['completeness_improvement']:+.1f}%\")\n",
    "print(f\"  ‚Ä¢ Duplicates removed: {quality_report['improvements']['duplicates_removed']}\")\n",
    "\n",
    "print(f\"\\nüîß Operations Performed:\")\n",
    "for i, operation in enumerate(cleaner.cleaning_log, 1):\n",
    "    print(f\"  {i}. {operation['step']}: {operation['details']}\")\n",
    "\n",
    "# ## 7. Export Cleaned Data\n",
    "\n",
    "print(f\"\\nüíæ EXPORTING CLEANED DATA...\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Export to multiple formats\n",
    "export_formats = {\n",
    "    'parquet': '../data/processed/cleaned_data.parquet',\n",
    "    'csv': '../data/processed/cleaned_data.csv',\n",
    "    'excel': '../data/processed/cleaned_data.xlsx'\n",
    "}\n",
    "\n",
    "for format_name, filepath in export_formats.items():\n",
    "    try:\n",
    "        if format_name == 'parquet':\n",
    "            df_clean.to_parquet(filepath, index=False)\n",
    "        elif format_name == 'csv':\n",
    "            df_clean.to_csv(filepath, index=False)\n",
    "        elif format_name == 'excel':\n",
    "            df_clean.to_excel(filepath, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Exported to {format_name.upper()}: {filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to export {format_name}: {e}\")\n",
    "\n",
    "# Export quality report\n",
    "try:\n",
    "    import json\n",
    "    with open('../data/processed/cleaning_report.json', 'w') as f:\n",
    "        # Convert datetime and other non-serializable objects\n",
    "        serializable_report = quality_report.copy()\n",
    "        serializable_report['cleaning_timestamp'] = str(quality_report['cleaning_timestamp'])\n",
    "        \n",
    "        # Convert numpy types to Python types\n",
    "        for key in ['original_data', 'cleaned_data']:\n",
    "            for subkey in ['data_types']:\n",
    "                serializable_report[key][subkey] = {\n",
    "                    k: str(v) for k, v in serializable_report[key][subkey].items()\n",
    "                }\n",
    "        \n",
    "        json.dump(serializable_report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Quality report exported: ../data/processed/cleaning_report.json\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to export quality report: {e}\")\n",
    "\n",
    "# ## 8. Summary Statistics for Cleaned Data\n",
    "\n",
    "print(f\"\\nüìà CLEANED DATA SUMMARY:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(f\"Final dataset shape: {df_clean.shape}\")\n",
    "print(f\"Columns: {list(df_clean.columns)}\")\n",
    "\n",
    "if 'emissions' in df_clean.columns:\n",
    "    emissions_summary = df_clean['emissions'].describe()\n",
    "    print(f\"\\nEmissions Statistics:\")\n",
    "    for stat, value in emissions_summary.items():\n",
    "        print(f\"  ‚Ä¢ {stat}: {value:.2f}\")\n",
    "\n",
    "# Top categories\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols[:3]:  # Show first 3 categorical columns\n",
    "    print(f\"\\nTop 5 {col}:\")\n",
    "    top_values = df_clean[col].value_counts().head()\n",
    "    for category, count in top_values.items():\n",
    "        pct = (count / len(df_clean)) * 100\n",
    "        print(f\"  ‚Ä¢ {category}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# ## 9. Data Cleaning Recommendations\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS FOR FUTURE DATA COLLECTION:\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "recommendations = [\n",
    "    \"Standardize country name formats across all data sources\",\n",
    "    \"Implement data validation rules to prevent negative emissions\",\n",
    "    \"Establish consistent date/year formatting (YYYY or YYYY-YYYY)\",\n",
    "    \"Create data dictionaries for categorical variables\",\n",
    "    \"Implement regular data quality checks during collection\",\n",
    "    \"Add data source metadata for traceability\",\n",
    "    \"Consider implementing automated outlier detection\",\n",
    "    \"Establish minimum required fields to reduce missing data\"\n",
    "]\n",
    "\n",
    "for i, recommendation in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {recommendation}\")\n",
    "\n",
    "print(f\"\\nüéâ DATA CLEANING COMPLETE!\")\n",
    "print(\"=\"*30)\n",
    "print(f\"‚ú® Clean dataset ready for analysis and visualization!\")\n",
    "print(f\"üìä {df_clean.shape[0]} records across {df_clean.shape[1]} features\")\n",
    "print(f\"üéØ Data quality: {validation_results.get('quality_score', 'Unknown')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
